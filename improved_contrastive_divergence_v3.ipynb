{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "improved_contrastive_divergence_v3.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karimul/improved_contrastive_divergence/blob/master/improved_contrastive_divergence_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYwSXttcfKdw"
      },
      "source": [
        "## Mounting to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hB0XyqO-Fn-E"
      },
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "ROOT = \"/content/drive/MyDrive/Colab Notebooks\"\n",
        "sample_dir = os.path.join(ROOT, 'improved_contrastive_divergence')\n",
        "if not os.path.exists(sample_dir):\n",
        "    os.makedirs(sample_dir)\n",
        "os.chdir(sample_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnrqsU3yfWjH"
      },
      "source": [
        "## Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ui-z8WnHcGOK"
      },
      "source": [
        "from easydict import EasyDict\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import timeit\n",
        "import os.path as osp\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import pickle\n",
        "from imageio import imread\n",
        "import cv2\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.nn import Dropout\n",
        "from torch.optim import Adam, SGD\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhmUyu8ZDhFW"
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBe5oB41farJ"
      },
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nvp4vS89cMLe"
      },
      "source": [
        "flags = EasyDict()\n",
        "\n",
        "# Configurations for distributed training\n",
        "flags['slurm'] = False # whether we are on slurm\n",
        "flags['repel_im'] = True # maximize entropy by repeling images from each other\n",
        "flags['hmc'] = False # use the hamiltonian monte carlo sampler\n",
        "flags['asgld'] = True # use the adaptively precondition SGLD sampler\n",
        "flags['square_energy'] = False # make the energy square\n",
        "flags['alias'] = False # make the energy square\n",
        "flags['cpu'] = torch.device(\"cpu\")\n",
        "flags['gpu'] = torch.device(\"cuda:0\")\n",
        "\n",
        "flags['dataset'] = 'cats' # cifar10 or celeba\n",
        "flags['batch_size'] = 128 #128 # batch size during training\n",
        "flags['multiscale'] = True # A multiscale EBM\n",
        "flags['self_attn'] = True #Use self attention in models\n",
        "flags['sigmoid'] = False # Apply sigmoid on energy (can improve the stability)\n",
        "flags['anneal'] = False # Decrease noise over Langevin steps\n",
        "flags['data_workers'] = 4 # Number of different data workers to load data in parallel\n",
        "flags['buffer_size'] = 10000 # Size of inputs\n",
        "\n",
        "# General Experiment Settings\n",
        "flags['exp'] = 'default' #name of experiments\n",
        "flags['log_interval'] = 50 #log outputs every so many batches\n",
        "flags['save_interval'] = 500 # save outputs every so many batches\n",
        "flags['test_interval'] = 500 # evaluate outputs every so many batches\n",
        "flags['resume_iter'] = 0 #iteration to resume training from\n",
        "flags['train'] = True # whether to train or test\n",
        "flags['transform'] = True # apply data augmentation when sampling from the replay buffer\n",
        "flags['kl'] = True # apply a KL term to loss\n",
        "flags['cuda'] = True # move device on cuda\n",
        "flags['epoch_num'] = 10000 # Number of Epochs to train on\n",
        "flags['ensembles'] = 1 #Number of ensembles to train models with\n",
        "flags['lr'] = 2e-4 #Learning for training\n",
        "flags['kl_coeff'] = 1.0 #coefficient for kl\n",
        "\n",
        "# EBM Specific Experiments Settings\n",
        "flags_objective = 'cd' #use the cd objective\n",
        "\n",
        "# Setting for MCMC sampling\n",
        "flags['num_steps'] = 40 # Steps of gradient descent for training\n",
        "flags['step_lr'] = 100.0 # Size of steps for gradient descent\n",
        "flags['replay_batch'] = True # Use MCMC chains initialized from a replay buffer.\n",
        "flags['reservoir'] = True # Use a reservoir of past entires\n",
        "flags['noise_scale'] = 1. # Relative amount of noise for MCMC\n",
        "flags['init_noise'] = 0.1\n",
        "flags['momentum'] = 0.9\n",
        "flags['eps'] = 1e-6\n",
        "flags['step_size'] = 10\n",
        "\n",
        "# Architecture Settings\n",
        "flags['filter_dim'] = 64 #64 #number of filters for conv nets\n",
        "flags['im_size'] = 32 #32 #size of images\n",
        "flags['spec_norm'] = False #Whether to use spectral normalization on weights\n",
        "flags['norm'] = True #Use group norm in models norm in models\n",
        "\n",
        "# Conditional settings\n",
        "flags['cond'] = False #conditional generation with the model\n",
        "flags['all_step'] = False #backprop through all langevin steps\n",
        "flags['log_grad'] = False #log the gradient norm of the kl term\n",
        "flags['cond_idx'] = 0 #conditioned index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDOMH-curbuQ"
      },
      "source": [
        "writer = SummaryWriter(comment=\"_ASGLD_{dataset}\".format(dataset=flags.dataset))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNsPJJm_gJy9"
      },
      "source": [
        "## Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hM3ARE-gaKQ"
      },
      "source": [
        "class Self_Attn(nn.Module):\n",
        "    \"\"\" Self attention Layer\"\"\"\n",
        "    def __init__(self,in_dim,activation):\n",
        "        super(Self_Attn,self).__init__()\n",
        "        self.chanel_in = in_dim\n",
        "        self.activation = activation\n",
        "\n",
        "        self.query_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim//8 , kernel_size= 1)\n",
        "        self.key_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim//8 , kernel_size= 1)\n",
        "        self.value_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim , kernel_size= 1)\n",
        "        self.gamma = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "        self.softmax  = nn.Softmax(dim=-1) #\n",
        "\n",
        "    def forward(self,x):\n",
        "        \"\"\"\n",
        "            inputs :\n",
        "                x : input feature maps( B X C X W X H)\n",
        "            returns :\n",
        "                out : self attention value + input feature\n",
        "                attention: B X N X N (N is Width*Height)\n",
        "        \"\"\"\n",
        "        m_batchsize,C,width ,height = x.size()\n",
        "        proj_query  = self.query_conv(x).view(m_batchsize,-1,width*height).permute(0,2,1) # B X CX(N)\n",
        "        proj_key =  self.key_conv(x).view(m_batchsize,-1,width*height) # B X C x (*W*H)\n",
        "        energy =  torch.bmm(proj_query,proj_key) # transpose check\n",
        "        attention = self.softmax(energy) # BX (N) X (N) \n",
        "        proj_value = self.value_conv(x).view(m_batchsize,-1,width*height) # B X C X N\n",
        "\n",
        "        out = torch.bmm(proj_value,attention.permute(0,2,1) )\n",
        "        out = out.view(m_batchsize,C,width,height)\n",
        "\n",
        "        out = self.gamma*out + x\n",
        "        return out,attention"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1kb8z4rDwar"
      },
      "source": [
        "class CondResBlock(nn.Module):\n",
        "    def __init__(self, args, downsample=True, rescale=True, filters=64, latent_dim=64, im_size=64, classes=512, norm=True, spec_norm=False):\n",
        "        super(CondResBlock, self).__init__()\n",
        "\n",
        "        self.filters = filters\n",
        "        self.latent_dim = latent_dim\n",
        "        self.im_size = im_size\n",
        "        self.downsample = downsample\n",
        "\n",
        "        if filters <= 128:\n",
        "            self.bn1 = nn.InstanceNorm2d(filters, affine=True)\n",
        "        else:\n",
        "            self.bn1 = nn.GroupNorm(32, filters)\n",
        "\n",
        "        if not norm:\n",
        "            self.bn1 = None\n",
        "\n",
        "        self.args = args\n",
        "\n",
        "        if spec_norm:\n",
        "            self.conv1 = spectral_norm(nn.Conv2d(filters, filters, kernel_size=3, stride=1, padding=1))\n",
        "        else:\n",
        "            self.conv1 = WSConv2d(filters, filters, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        if filters <= 128:\n",
        "            self.bn2 = nn.InstanceNorm2d(filters, affine=True)\n",
        "        else:\n",
        "            self.bn2 = nn.GroupNorm(32, filters, affine=True)\n",
        "\n",
        "        if not norm:\n",
        "            self.bn2 = None\n",
        "\n",
        "        if spec_norm:\n",
        "            self.conv2 = spectral_norm(nn.Conv2d(filters, filters, kernel_size=3, stride=1, padding=1))\n",
        "        else:\n",
        "            self.conv2 = WSConv2d(filters, filters, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        self.dropout = Dropout(0.2)\n",
        "\n",
        "        # Upscale to an mask of image\n",
        "        self.latent_map = nn.Linear(classes, 2*filters)\n",
        "        self.latent_map_2 = nn.Linear(classes, 2*filters)\n",
        "\n",
        "        self.relu = torch.nn.ReLU(inplace=True)\n",
        "        self.act = swish\n",
        "\n",
        "        # Upscale to mask of image\n",
        "        if downsample:\n",
        "            if rescale:\n",
        "                self.conv_downsample = nn.Conv2d(filters, 2 * filters, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "                if args.alias:\n",
        "                    self.avg_pool = Downsample(channels=2*filters)\n",
        "                else:\n",
        "                    self.avg_pool = nn.AvgPool2d(3, stride=2, padding=1)\n",
        "            else:\n",
        "                self.conv_downsample = nn.Conv2d(filters, filters, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "                if args.alias:\n",
        "                    self.avg_pool = Downsample(channels=filters)\n",
        "                else:\n",
        "                    self.avg_pool = nn.AvgPool2d(3, stride=2, padding=1)\n",
        "\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        x_orig = x\n",
        "\n",
        "        if y is not None:\n",
        "            latent_map = self.latent_map(y).view(-1, 2*self.filters, 1, 1)\n",
        "\n",
        "            gain = latent_map[:, :self.filters]\n",
        "            bias = latent_map[:, self.filters:]\n",
        "\n",
        "        x = self.conv1(x)\n",
        "\n",
        "        if self.bn1 is not None:\n",
        "            x = self.bn1(x)\n",
        "\n",
        "        if y is not None:\n",
        "            x = gain * x + bias\n",
        "\n",
        "        x = self.act(x)\n",
        "\n",
        "        if y is not None:\n",
        "            latent_map = self.latent_map_2(y).view(-1, 2*self.filters, 1, 1)\n",
        "            gain = latent_map[:, :self.filters]\n",
        "            bias = latent_map[:, self.filters:]\n",
        "\n",
        "        x = self.conv2(x)\n",
        "\n",
        "        if self.bn2 is not None:\n",
        "            x = self.bn2(x)\n",
        "\n",
        "        if y is not None:\n",
        "            x = gain * x + bias\n",
        "\n",
        "        x = self.act(x)\n",
        "\n",
        "        x_out = x\n",
        "\n",
        "        if self.downsample:\n",
        "            x_out = self.conv_downsample(x_out)\n",
        "            x_out = self.act(self.avg_pool(x_out))\n",
        "\n",
        "        return x_out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CG8SD8lfjay"
      },
      "source": [
        "## MNIST Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H34FfHFUcW-m"
      },
      "source": [
        "class MNISTModel(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(MNISTModel, self).__init__()\n",
        "        self.act = swish\n",
        "        # self.relu = torch.nn.ReLU(inplace=True)\n",
        "\n",
        "        self.args = args\n",
        "        self.filter_dim = args.filter_dim\n",
        "        self.init_main_model()\n",
        "        self.init_label_map()\n",
        "        self.filter_dim = args.filter_dim\n",
        "\n",
        "        # self.act = self.relu\n",
        "        self.cond = args.cond\n",
        "        self.sigmoid = args.sigmoid\n",
        "\n",
        "\n",
        "    def init_main_model(self):\n",
        "        args = self.args\n",
        "        filter_dim = self.filter_dim\n",
        "        im_size = 28\n",
        "        self.conv1 = nn.Conv2d(1, filter_dim, kernel_size=3, stride=1, padding=1)\n",
        "        self.res1 = CondResBlock(args, filters=filter_dim, latent_dim=1, im_size=im_size)\n",
        "        self.res2 = CondResBlock(args, filters=2*filter_dim, latent_dim=1, im_size=im_size)\n",
        "\n",
        "        self.res3 = CondResBlock(args, filters=4*filter_dim, latent_dim=1, im_size=im_size)\n",
        "        self.energy_map = nn.Linear(filter_dim*8, 1)\n",
        "\n",
        "\n",
        "    def init_label_map(self):\n",
        "        args = self.args\n",
        "\n",
        "        self.map_fc1 = nn.Linear(10, 256)\n",
        "        self.map_fc2 = nn.Linear(256, 256)\n",
        "\n",
        "    def main_model(self, x, latent):\n",
        "        x = x.view(-1, 1, 28, 28)\n",
        "        x = self.act(self.conv1(x))\n",
        "        x = self.res1(x, latent)\n",
        "        x = self.res2(x, latent)\n",
        "        x = self.res3(x, latent)\n",
        "        x = self.act(x)\n",
        "        x = x.mean(dim=2).mean(dim=2)\n",
        "        energy = self.energy_map(x)\n",
        "\n",
        "        return energy\n",
        "\n",
        "    def label_map(self, latent):\n",
        "        x = self.act(self.map_fc1(latent))\n",
        "        x = self.map_fc2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, x, latent):\n",
        "        args = self.args\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        if self.cond:\n",
        "            latent = self.label_map(latent)\n",
        "        else:\n",
        "            latent = None\n",
        "\n",
        "        energy = self.main_model(x, latent)\n",
        "\n",
        "        return energy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MI6_NLOfmtx"
      },
      "source": [
        "## CelebA Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8p2DmIlkfpIQ"
      },
      "source": [
        "class CelebAModel(nn.Module):\n",
        "    def __init__(self, args, debug=False):\n",
        "        super(CelebAModel, self).__init__()\n",
        "        self.act = swish\n",
        "        self.max_pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.cond = args.cond\n",
        "\n",
        "        self.args = args\n",
        "        self.init_main_model()\n",
        "\n",
        "        if args.multiscale:\n",
        "            self.init_mid_model()\n",
        "            self.init_small_model()\n",
        "\n",
        "        self.relu = torch.nn.ReLU(inplace=True)\n",
        "        self.downsample = Downsample(channels=3)\n",
        "        self.heir_weight = nn.Parameter(torch.Tensor([1.0, 1.0, 1.0]))\n",
        "        self.debug = debug\n",
        "\n",
        "    def init_main_model(self):\n",
        "        args = self.args\n",
        "        filter_dim = args.filter_dim\n",
        "        latent_dim = args.filter_dim\n",
        "        im_size = args.im_size\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, filter_dim // 2, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        self.res_1a = CondResBlock(args, filters=filter_dim // 2, latent_dim=latent_dim, im_size=im_size, downsample=True, classes=2, norm=args.norm, spec_norm=args.spec_norm)\n",
        "        self.res_1b = CondResBlock(args, filters=filter_dim, latent_dim=latent_dim, im_size=im_size, rescale=False, classes=2, norm=args.norm, spec_norm=args.spec_norm)\n",
        "\n",
        "        self.res_2a = CondResBlock(args, filters=filter_dim, latent_dim=latent_dim, im_size=im_size, downsample=True, rescale=False, classes=2, norm=args.norm, spec_norm=args.spec_norm)\n",
        "        self.res_2b = CondResBlock(args, filters=filter_dim, latent_dim=latent_dim, im_size=im_size, rescale=True, classes=2, norm=args.norm, spec_norm=args.spec_norm)\n",
        "\n",
        "        self.res_3a = CondResBlock(args, filters=2*filter_dim, latent_dim=latent_dim, im_size=im_size, downsample=False, classes=2, norm=args.norm, spec_norm=args.spec_norm)\n",
        "        self.res_3b = CondResBlock(args, filters=2*filter_dim, latent_dim=latent_dim, im_size=im_size, rescale=True, classes=2, norm=args.norm, spec_norm=args.spec_norm)\n",
        "\n",
        "        self.res_4a = CondResBlock(args, filters=4*filter_dim, latent_dim=latent_dim, im_size=im_size, downsample=False, classes=2, norm=args.norm, spec_norm=args.spec_norm)\n",
        "        self.res_4b = CondResBlock(args, filters=4*filter_dim, latent_dim=latent_dim, im_size=im_size, rescale=True, classes=2, norm=args.norm, spec_norm=args.spec_norm)\n",
        "\n",
        "        self.self_attn = Self_Attn(4 * filter_dim, self.act)\n",
        "\n",
        "        self.energy_map = nn.Linear(filter_dim*8, 1)\n",
        "\n",
        "    def init_mid_model(self):\n",
        "        args = self.args\n",
        "        filter_dim = args.filter_dim\n",
        "        latent_dim = args.filter_dim\n",
        "        im_size = args.im_size\n",
        "\n",
        "        self.mid_conv1 = nn.Conv2d(3, filter_dim, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        self.mid_res_1a = CondResBlock(args, filters=filter_dim, latent_dim=latent_dim, im_size=im_size, downsample=True, rescale=False, classes=2)\n",
        "        self.mid_res_1b = CondResBlock(args, filters=filter_dim, latent_dim=latent_dim, im_size=im_size, rescale=False, classes=2)\n",
        "\n",
        "        self.mid_res_2a = CondResBlock(args, filters=filter_dim, latent_dim=latent_dim, im_size=im_size, downsample=True, rescale=False, classes=2)\n",
        "        self.mid_res_2b = CondResBlock(args, filters=filter_dim, latent_dim=latent_dim, im_size=im_size, rescale=True, classes=2)\n",
        "\n",
        "        self.mid_res_3a = CondResBlock(args, filters=2*filter_dim, latent_dim=latent_dim, im_size=im_size, downsample=False, classes=2)\n",
        "        self.mid_res_3b = CondResBlock(args, filters=2*filter_dim, latent_dim=latent_dim, im_size=im_size, rescale=True, classes=2)\n",
        "\n",
        "        self.mid_energy_map = nn.Linear(filter_dim*4, 1)\n",
        "        self.avg_pool = Downsample(channels=3)\n",
        "\n",
        "    def init_small_model(self):\n",
        "        args = self.args\n",
        "        filter_dim = args.filter_dim\n",
        "        latent_dim = args.filter_dim\n",
        "        im_size = args.im_size\n",
        "\n",
        "        self.small_conv1 = nn.Conv2d(3, filter_dim, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        self.small_res_1a = CondResBlock(args, filters=filter_dim, latent_dim=latent_dim, im_size=im_size, downsample=True, rescale=False, classes=2)\n",
        "        self.small_res_1b = CondResBlock(args, filters=filter_dim, latent_dim=latent_dim, im_size=im_size, rescale=False, classes=2)\n",
        "\n",
        "        self.small_res_2a = CondResBlock(args, filters=filter_dim, latent_dim=latent_dim, im_size=im_size, downsample=True, rescale=False, classes=2)\n",
        "        self.small_res_2b = CondResBlock(args, filters=filter_dim, latent_dim=latent_dim, im_size=im_size, rescale=True, classes=2)\n",
        "\n",
        "        self.small_energy_map = nn.Linear(filter_dim*2, 1)\n",
        "\n",
        "    def main_model(self, x, latent):\n",
        "        x = self.act(self.conv1(x))\n",
        "\n",
        "        x = self.res_1a(x, latent)\n",
        "        x = self.res_1b(x, latent)\n",
        "\n",
        "        x = self.res_2a(x, latent)\n",
        "        x = self.res_2b(x, latent)\n",
        "\n",
        "\n",
        "        x = self.res_3a(x, latent)\n",
        "        x = self.res_3b(x, latent)\n",
        "\n",
        "        if self.args.self_attn:\n",
        "            x, _ = self.self_attn(x)\n",
        "\n",
        "        x = self.res_4a(x, latent)\n",
        "        x = self.res_4b(x, latent)\n",
        "        x = self.act(x)\n",
        "\n",
        "        x = x.mean(dim=2).mean(dim=2)\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "        energy = self.energy_map(x)\n",
        "\n",
        "        if self.args.square_energy:\n",
        "            energy = torch.pow(energy, 2)\n",
        "\n",
        "        if self.args.sigmoid:\n",
        "            energy = F.sigmoid(energy)\n",
        "\n",
        "        return energy\n",
        "\n",
        "    def mid_model(self, x, latent):\n",
        "        x = F.avg_pool2d(x, 3, stride=2, padding=1)\n",
        "\n",
        "        x = self.act(self.mid_conv1(x))\n",
        "\n",
        "        x = self.mid_res_1a(x, latent)\n",
        "        x = self.mid_res_1b(x, latent)\n",
        "\n",
        "        x = self.mid_res_2a(x, latent)\n",
        "        x = self.mid_res_2b(x, latent)\n",
        "\n",
        "        x = self.mid_res_3a(x, latent)\n",
        "        x = self.mid_res_3b(x, latent)\n",
        "        x = self.act(x)\n",
        "\n",
        "        x = x.mean(dim=2).mean(dim=2)\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "        energy = self.mid_energy_map(x)\n",
        "\n",
        "        if self.args.square_energy:\n",
        "            energy = torch.pow(energy, 2)\n",
        "\n",
        "        if self.args.sigmoid:\n",
        "            energy = F.sigmoid(energy)\n",
        "\n",
        "        return energy\n",
        "\n",
        "    def small_model(self, x, latent):\n",
        "        x = F.avg_pool2d(x, 3, stride=2, padding=1)\n",
        "        x = F.avg_pool2d(x, 3, stride=2, padding=1)\n",
        "\n",
        "        x = self.act(self.small_conv1(x))\n",
        "\n",
        "        x = self.small_res_1a(x, latent)\n",
        "        x = self.small_res_1b(x, latent)\n",
        "\n",
        "        x = self.small_res_2a(x, latent)\n",
        "        x = self.small_res_2b(x, latent)\n",
        "        x = self.act(x)\n",
        "\n",
        "        x = x.mean(dim=2).mean(dim=2)\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "        energy = self.small_energy_map(x)\n",
        "\n",
        "        if self.args.square_energy:\n",
        "            energy = torch.pow(energy, 2)\n",
        "\n",
        "        if self.args.sigmoid:\n",
        "            energy = F.sigmoid(energy)\n",
        "\n",
        "        return energy\n",
        "\n",
        "    def label_map(self, latent):\n",
        "        x = self.act(self.map_fc1(latent))\n",
        "        x = self.act(self.map_fc2(x))\n",
        "        x = self.act(self.map_fc3(x))\n",
        "        x = self.act(self.map_fc4(x))\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, x, latent):\n",
        "        args = self.args\n",
        "\n",
        "        if not self.cond:\n",
        "            latent = None\n",
        "\n",
        "        energy = self.main_model(x, latent)\n",
        "\n",
        "        if args.multiscale:\n",
        "            large_energy = energy\n",
        "            mid_energy = self.mid_model(x, latent)\n",
        "            small_energy = self.small_model(x, latent)\n",
        "            energy = torch.cat([small_energy, mid_energy, large_energy], dim=-1)\n",
        "\n",
        "        return energy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1rlR1022jzH"
      },
      "source": [
        "## ResNet Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ccb-o1li2loe"
      },
      "source": [
        "class ResNetModel(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(ResNetModel, self).__init__()\n",
        "        self.act = swish\n",
        "\n",
        "        self.args = args\n",
        "        self.spec_norm = args.spec_norm\n",
        "        self.norm = args.norm\n",
        "        self.init_main_model()\n",
        "\n",
        "        if args.multiscale:\n",
        "            self.init_mid_model()\n",
        "            self.init_small_model()\n",
        "\n",
        "        self.relu = torch.nn.ReLU(inplace=True)\n",
        "        self.downsample = Downsample(channels=3)\n",
        "\n",
        "        self.cond = args.cond\n",
        "\n",
        "    def init_main_model(self):\n",
        "        args = self.args\n",
        "        filter_dim = args.filter_dim\n",
        "        latent_dim = args.filter_dim\n",
        "        im_size = args.im_size\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, filter_dim, kernel_size=3, stride=1, padding=1)\n",
        "        self.res_1a = CondResBlock(args, filters=filter_dim, latent_dim=latent_dim, im_size=im_size, downsample=False, spec_norm=self.spec_norm, norm=self.norm)\n",
        "        self.res_1b = CondResBlock(args, filters=filter_dim, latent_dim=latent_dim, im_size=im_size, rescale=False, spec_norm=self.spec_norm, norm=self.norm)\n",
        "\n",
        "        self.res_2a = CondResBlock(args, filters=filter_dim, latent_dim=latent_dim, im_size=im_size, downsample=False, spec_norm=self.spec_norm, norm=self.norm)\n",
        "        self.res_2b = CondResBlock(args, filters=filter_dim, latent_dim=latent_dim, im_size=im_size, rescale=True, spec_norm=self.spec_norm, norm=self.norm)\n",
        "\n",
        "        self.res_3a = CondResBlock(args, filters=2*filter_dim, latent_dim=latent_dim, im_size=im_size, downsample=False, spec_norm=self.spec_norm, norm=self.norm)\n",
        "        self.res_3b = CondResBlock(args, filters=2*filter_dim, latent_dim=latent_dim, im_size=im_size, rescale=True, spec_norm=self.spec_norm, norm=self.norm)\n",
        "\n",
        "        self.res_4a = CondResBlock(args, filters=4*filter_dim, latent_dim=latent_dim, im_size=im_size, downsample=False, spec_norm=self.spec_norm, norm=self.norm)\n",
        "        self.res_4b = CondResBlock(args, filters=4*filter_dim, latent_dim=latent_dim, im_size=im_size, rescale=True, spec_norm=self.spec_norm, norm=self.norm)\n",
        "\n",
        "        self.self_attn = Self_Attn(2 * filter_dim, self.act)\n",
        "\n",
        "        self.energy_map = nn.Linear(filter_dim*8, 1)\n",
        "\n",
        "    def init_mid_model(self):\n",
        "        args = self.args\n",
        "        filter_dim = args.filter_dim\n",
        "        latent_dim = args.filter_dim\n",
        "        im_size = args.im_size\n",
        "\n",
        "        self.mid_conv1 = nn.Conv2d(3, filter_dim, kernel_size=3, stride=1, padding=1)\n",
        "        self.mid_res_1a = CondResBlock(args, filters=filter_dim, latent_dim=latent_dim, im_size=im_size, downsample=False, spec_norm=self.spec_norm, norm=self.norm)\n",
        "        self.mid_res_1b = CondResBlock(args, filters=filter_dim, latent_dim=latent_dim, im_size=im_size, rescale=False, spec_norm=self.spec_norm, norm=self.norm)\n",
        "\n",
        "        self.mid_res_2a = CondResBlock(args, filters=filter_dim, latent_dim=latent_dim, im_size=im_size, downsample=False, spec_norm=self.spec_norm, norm=self.norm)\n",
        "        self.mid_res_2b = CondResBlock(args, filters=filter_dim, latent_dim=latent_dim, im_size=im_size, rescale=True, spec_norm=self.spec_norm, norm=self.norm)\n",
        "\n",
        "        self.mid_res_3a = CondResBlock(args, filters=2*filter_dim, latent_dim=latent_dim, im_size=im_size, downsample=False, spec_norm=self.spec_norm, norm=self.norm)\n",
        "        self.mid_res_3b = CondResBlock(args, filters=2*filter_dim, latent_dim=latent_dim, im_size=im_size, rescale=True, spec_norm=self.spec_norm, norm=self.norm)\n",
        "\n",
        "        self.mid_energy_map = nn.Linear(filter_dim*4, 1)\n",
        "        self.avg_pool = Downsample(channels=3)\n",
        "\n",
        "    def init_small_model(self):\n",
        "        args = self.args\n",
        "        filter_dim = args.filter_dim\n",
        "        latent_dim = args.filter_dim\n",
        "        im_size = args.im_size\n",
        "\n",
        "        self.small_conv1 = nn.Conv2d(3, filter_dim, kernel_size=3, stride=1, padding=1)\n",
        "        self.small_res_1a = CondResBlock(args, filters=filter_dim, latent_dim=latent_dim, im_size=im_size, downsample=False, spec_norm=self.spec_norm, norm=self.norm)\n",
        "        self.small_res_1b = CondResBlock(args, filters=filter_dim, latent_dim=latent_dim, im_size=im_size, rescale=False, spec_norm=self.spec_norm, norm=self.norm)\n",
        "\n",
        "        self.small_res_2a = CondResBlock(args, filters=filter_dim, latent_dim=latent_dim, im_size=im_size, downsample=False, spec_norm=self.spec_norm, norm=self.norm)\n",
        "        self.small_res_2b = CondResBlock(args, filters=filter_dim, latent_dim=latent_dim, im_size=im_size, rescale=True, spec_norm=self.spec_norm, norm=self.norm)\n",
        "\n",
        "        self.small_energy_map = nn.Linear(filter_dim*2, 1)\n",
        "\n",
        "    def main_model(self, x, latent, compute_feat=False):\n",
        "        x = self.act(self.conv1(x))\n",
        "\n",
        "        x = self.res_1a(x, latent)\n",
        "        x = self.res_1b(x, latent)\n",
        "\n",
        "        x = self.res_2a(x, latent)\n",
        "        x = self.res_2b(x, latent)\n",
        "\n",
        "        if self.args.self_attn:\n",
        "            x, _ = self.self_attn(x)\n",
        "\n",
        "        x = self.res_3a(x, latent)\n",
        "        x = self.res_3b(x, latent)\n",
        "\n",
        "        x = self.res_4a(x, latent)\n",
        "        x = self.res_4b(x, latent)\n",
        "        x = self.act(x)\n",
        "\n",
        "        x = x.mean(dim=2).mean(dim=2)\n",
        "\n",
        "        if compute_feat:\n",
        "            return x\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "        energy = self.energy_map(x)\n",
        "\n",
        "        if self.args.square_energy:\n",
        "            energy = torch.pow(energy, 2)\n",
        "\n",
        "        if self.args.sigmoid:\n",
        "            energy = F.sigmoid(energy)\n",
        "\n",
        "        return energy\n",
        "\n",
        "    def mid_model(self, x, latent):\n",
        "        x = F.avg_pool2d(x, 3, stride=2, padding=1)\n",
        "\n",
        "        x = self.act(self.mid_conv1(x))\n",
        "\n",
        "        x = self.mid_res_1a(x, latent)\n",
        "        x = self.mid_res_1b(x, latent)\n",
        "\n",
        "        x = self.mid_res_2a(x, latent)\n",
        "        x = self.mid_res_2b(x, latent)\n",
        "\n",
        "        x = self.mid_res_3a(x, latent)\n",
        "        x = self.mid_res_3b(x, latent)\n",
        "        x = self.act(x)\n",
        "\n",
        "        x = x.mean(dim=2).mean(dim=2)\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "        energy = self.mid_energy_map(x)\n",
        "\n",
        "        if self.args.square_energy:\n",
        "            energy = torch.pow(energy, 2)\n",
        "\n",
        "        if self.args.sigmoid:\n",
        "            energy = F.sigmoid(energy)\n",
        "\n",
        "        return energy\n",
        "\n",
        "    def small_model(self, x, latent):\n",
        "        x = F.avg_pool2d(x, 3, stride=2, padding=1)\n",
        "        x = F.avg_pool2d(x, 3, stride=2, padding=1)\n",
        "\n",
        "        x = self.act(self.small_conv1(x))\n",
        "\n",
        "        x = self.small_res_1a(x, latent)\n",
        "        x = self.small_res_1b(x, latent)\n",
        "\n",
        "        x = self.small_res_2a(x, latent)\n",
        "        x = self.small_res_2b(x, latent)\n",
        "        x = self.act(x)\n",
        "\n",
        "        x = x.mean(dim=2).mean(dim=2)\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "        energy = self.small_energy_map(x)\n",
        "\n",
        "        if self.args.square_energy:\n",
        "            energy = torch.pow(energy, 2)\n",
        "\n",
        "        if self.args.sigmoid:\n",
        "            energy = F.sigmoid(energy)\n",
        "\n",
        "        return energy\n",
        "\n",
        "    def forward(self, x, latent):\n",
        "        args = self.args\n",
        "\n",
        "        if self.cond:\n",
        "            latent = self.label_map(latent)\n",
        "        else:\n",
        "            latent = None\n",
        "\n",
        "        energy = self.main_model(x, latent)\n",
        "\n",
        "        if args.multiscale:\n",
        "            large_energy = energy\n",
        "            mid_energy = self.mid_model(x, latent)\n",
        "            small_energy = self.small_model(x, latent)\n",
        "\n",
        "            # Add a seperate energy penalizing the different energies from each model\n",
        "            energy = torch.cat([small_energy, mid_energy, large_energy], dim=-1)\n",
        "\n",
        "        return energy\n",
        "\n",
        "    def compute_feat(self, x, latent):\n",
        "        return self.main_model(x, None, compute_feat=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Apumq9FJMqxU"
      },
      "source": [
        "## Downsample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kt9kHpSkMsMa"
      },
      "source": [
        "class Downsample(nn.Module):\n",
        "    def __init__(self, pad_type='reflect', filt_size=3, stride=2, channels=None, pad_off=0):\n",
        "        super(Downsample, self).__init__()\n",
        "        self.filt_size = filt_size\n",
        "        self.pad_off = pad_off\n",
        "        self.pad_sizes = [int(1.*(filt_size-1)/2), int(np.ceil(1.*(filt_size-1)/2)), int(1.*(filt_size-1)/2), int(np.ceil(1.*(filt_size-1)/2))]\n",
        "        self.pad_sizes = [pad_size+pad_off for pad_size in self.pad_sizes]\n",
        "        self.stride = stride\n",
        "        self.off = int((self.stride-1)/2.)\n",
        "        self.channels = channels\n",
        "\n",
        "        if(self.filt_size==1):\n",
        "            a = np.array([1.,])\n",
        "        elif(self.filt_size==2):\n",
        "            a = np.array([1., 1.])\n",
        "        elif(self.filt_size==3):\n",
        "            a = np.array([1., 2., 1.])\n",
        "        elif(self.filt_size==4):\n",
        "            a = np.array([1., 3., 3., 1.])\n",
        "        elif(self.filt_size==5):\n",
        "            a = np.array([1., 4., 6., 4., 1.])\n",
        "        elif(self.filt_size==6):\n",
        "            a = np.array([1., 5., 10., 10., 5., 1.])\n",
        "        elif(self.filt_size==7):\n",
        "            a = np.array([1., 6., 15., 20., 15., 6., 1.])\n",
        "\n",
        "        filt = torch.Tensor(a[:,None]*a[None,:])\n",
        "        filt = filt/torch.sum(filt)\n",
        "        self.register_buffer('filt', filt[None,None,:,:].repeat((self.channels,1,1,1)))\n",
        "\n",
        "        self.pad = get_pad_layer(pad_type)(self.pad_sizes)\n",
        "\n",
        "    def forward(self, inp):\n",
        "        if(self.filt_size==1):\n",
        "            if(self.pad_off==0):\n",
        "                return inp[:,:,::self.stride,::self.stride]\n",
        "            else:\n",
        "                return self.pad(inp)[:,:,::self.stride,::self.stride]\n",
        "        else:\n",
        "            return F.conv2d(self.pad(inp), self.filt, stride=self.stride, groups=inp.shape[1])\n",
        "\n",
        "def get_pad_layer(pad_type):\n",
        "    if(pad_type in ['refl','reflect']):\n",
        "        PadLayer = nn.ReflectionPad2d\n",
        "    elif(pad_type in ['repl','replicate']):\n",
        "        PadLayer = nn.ReplicationPad2d\n",
        "    elif(pad_type=='zero'):\n",
        "        PadLayer = nn.ZeroPad2d\n",
        "    else:\n",
        "        print('Pad type [%s] not recognized'%pad_type)\n",
        "    return PadLayer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiIqNn3yf2Lo"
      },
      "source": [
        "## Replay Buffer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kjqZHkvgCen"
      },
      "source": [
        "class GaussianBlur(object):\n",
        "\n",
        "    def __init__(self, min=0.1, max=2.0, kernel_size=9):\n",
        "        self.min = min\n",
        "        self.max = max\n",
        "        self.kernel_size = kernel_size\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        sample = np.array(sample)\n",
        "\n",
        "        # blur the image with a 50% chance\n",
        "        prob = np.random.random_sample()\n",
        "\n",
        "        if prob < 0.5:\n",
        "            sigma = (self.max - self.min) * np.random.random_sample() + self.min\n",
        "            sample = cv2.GaussianBlur(sample, (self.kernel_size, self.kernel_size), sigma)\n",
        "\n",
        "        return sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56N6PS_agDLK"
      },
      "source": [
        "class ReplayBuffer(object):\n",
        "    def __init__(self, size, transform, dataset):\n",
        "        \"\"\"Create Replay buffer.\n",
        "        Parameters\n",
        "        ----------\n",
        "        size: int\n",
        "            Max number of transitions to store in the buffer. When the buffer\n",
        "            overflows the old memories are dropped.\n",
        "        \"\"\"\n",
        "        self._storage = []\n",
        "        self._maxsize = size\n",
        "        self._next_idx = 0\n",
        "        self.gaussian_blur = GaussianBlur()\n",
        "\n",
        "        def get_color_distortion(s=1.0):\n",
        "        # s is the strength of color distortion.\n",
        "            color_jitter = transforms.ColorJitter(0.8*s, 0.8*s, 0.8*s, 0.4*s)\n",
        "            rnd_color_jitter = transforms.RandomApply([color_jitter], p=0.8)\n",
        "            rnd_gray = transforms.RandomGrayscale(p=0.2)\n",
        "            color_distort = transforms.Compose([\n",
        "                rnd_color_jitter,\n",
        "                rnd_gray])\n",
        "            return color_distort\n",
        "\n",
        "        color_transform = get_color_distortion()\n",
        "\n",
        "        if dataset in (\"cifar10\", \"celeba\", \"cats\"):\n",
        "            im_size = 32\n",
        "        elif dataset == \"continual\":\n",
        "            im_size = 64\n",
        "        elif dataset == \"celebahq\":\n",
        "            im_size = 128\n",
        "        elif dataset == \"object\":\n",
        "            im_size = 128\n",
        "        elif dataset == \"mnist\":\n",
        "            im_size = 28\n",
        "        elif dataset == \"moving_mnist\":\n",
        "            im_size = 28\n",
        "        elif dataset == \"imagenet\":\n",
        "            im_size = 128\n",
        "        elif dataset == \"lsun\":\n",
        "            im_size = 128\n",
        "        else:\n",
        "            assert False\n",
        "\n",
        "        self.dataset = dataset\n",
        "        if transform:\n",
        "            if dataset in (\"cifar10\", \"celeba\", \"cats\"):\n",
        "                self.transform = transforms.Compose([transforms.RandomResizedCrop(im_size, scale=(0.08, 1.0)), transforms.RandomHorizontalFlip(), color_transform, transforms.ToTensor()])\n",
        "            elif dataset == \"continual\":\n",
        "                color_transform = get_color_distortion(0.1)\n",
        "                self.transform = transforms.Compose([transforms.RandomResizedCrop(im_size, scale=(0.7, 1.0)), color_transform, transforms.ToTensor()])\n",
        "            elif dataset == \"celebahq\":\n",
        "                self.transform = transforms.Compose([transforms.RandomResizedCrop(im_size, scale=(0.08, 1.0)), transforms.RandomHorizontalFlip(), color_transform, transforms.ToTensor()])\n",
        "            elif dataset == \"imagenet\":\n",
        "                self.transform = transforms.Compose([transforms.RandomResizedCrop(im_size, scale=(0.01, 1.0)), transforms.RandomHorizontalFlip(), color_transform, transforms.ToTensor()])\n",
        "            elif dataset == \"object\":\n",
        "                self.transform = transforms.Compose([transforms.RandomResizedCrop(im_size, scale=(0.01, 1.0)), transforms.RandomHorizontalFlip(), color_transform, transforms.ToTensor()])\n",
        "            elif dataset == \"lsun\":\n",
        "                self.transform = transforms.Compose([transforms.RandomResizedCrop(im_size, scale=(0.08, 1.0)), transforms.RandomHorizontalFlip(), color_transform, transforms.ToTensor()])\n",
        "            elif dataset == \"mnist\":\n",
        "                self.transform = None\n",
        "            elif dataset == \"moving_mnist\":\n",
        "                self.transform = None\n",
        "            else:\n",
        "                assert False\n",
        "        else:\n",
        "            self.transform = None\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._storage)\n",
        "\n",
        "    def add(self, ims):\n",
        "        batch_size = ims.shape[0]\n",
        "        if self._next_idx >= len(self._storage):\n",
        "            self._storage.extend(list(ims))\n",
        "        else:\n",
        "            if batch_size + self._next_idx < self._maxsize:\n",
        "                self._storage[self._next_idx:self._next_idx +\n",
        "                              batch_size] = list(ims)\n",
        "            else:\n",
        "                split_idx = self._maxsize - self._next_idx\n",
        "                self._storage[self._next_idx:] = list(ims)[:split_idx]\n",
        "                self._storage[:batch_size - split_idx] = list(ims)[split_idx:]\n",
        "        self._next_idx = (self._next_idx + ims.shape[0]) % self._maxsize\n",
        "\n",
        "    def _encode_sample(self, idxes, no_transform=False, downsample=False):\n",
        "        ims = []\n",
        "        for i in idxes:\n",
        "            im = self._storage[i]\n",
        "\n",
        "            if self.dataset != \"mnist\":\n",
        "                if (self.transform is not None) and (not no_transform):\n",
        "                    im = im.transpose((1, 2, 0))\n",
        "                    im = np.array(self.transform(Image.fromarray(np.array(im))))\n",
        "\n",
        "                # if downsample and (self.dataset in [\"celeba\", \"object\", \"imagenet\"]):\n",
        "                #     im = im[:, ::4, ::4]\n",
        "\n",
        "            im = im * 255\n",
        "            ims.append(im)\n",
        "        return np.array(ims)\n",
        "\n",
        "    def sample(self, batch_size, no_transform=False, downsample=False):\n",
        "        \"\"\"Sample a batch of experiences.\n",
        "        Parameters\n",
        "        ----------\n",
        "        batch_size: int\n",
        "            How many transitions to sample.\n",
        "        Returns\n",
        "        -------\n",
        "        obs_batch: np.array\n",
        "            batch of observations\n",
        "        act_batch: np.array\n",
        "            batch of actions executed given obs_batch\n",
        "        rew_batch: np.array\n",
        "            rewards received as results of executing act_batch\n",
        "        next_obs_batch: np.array\n",
        "            next set of observations seen after executing act_batch\n",
        "        done_mask: np.array\n",
        "            done_mask[i] = 1 if executing act_batch[i] resulted in\n",
        "            the end of an episode and 0 otherwise.\n",
        "        \"\"\"\n",
        "        idxes = [random.randint(0, len(self._storage) - 1)\n",
        "                 for _ in range(batch_size)]\n",
        "        return self._encode_sample(idxes, no_transform=no_transform, downsample=downsample), idxes\n",
        "\n",
        "    def set_elms(self, data, idxes):\n",
        "        if len(self._storage) < self._maxsize:\n",
        "            self.add(data)\n",
        "        else:\n",
        "            for i, ix in enumerate(idxes):\n",
        "                self._storage[ix] = data[i]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXPbRT7wgGEW"
      },
      "source": [
        "class ReservoirBuffer(object):\n",
        "    def __init__(self, size, transform, dataset):\n",
        "        \"\"\"Create Replay buffer.\n",
        "        Parameters\n",
        "        ----------\n",
        "        size: int\n",
        "            Max number of transitions to store in the buffer. When the buffer\n",
        "            overflows the old memories are dropped.\n",
        "        \"\"\"\n",
        "        self._storage = []\n",
        "        self._maxsize = size\n",
        "        self._next_idx = 0\n",
        "        self.n = 0\n",
        "\n",
        "        def get_color_distortion(s=1.0):\n",
        "        # s is the strength of color distortion.\n",
        "            color_jitter = transforms.ColorJitter(0.8*s, 0.8*s, 0.8*s, 0.4*s)\n",
        "            rnd_color_jitter = transforms.RandomApply([color_jitter], p=0.8)\n",
        "            rnd_gray = transforms.RandomGrayscale(p=0.2)\n",
        "            color_distort = transforms.Compose([\n",
        "                rnd_color_jitter,\n",
        "                rnd_gray])\n",
        "            return color_distort\n",
        "\n",
        "        if dataset in (\"cifar10\", \"celeba\", \"cats\"):\n",
        "            im_size = 32\n",
        "        elif dataset == \"continual\":\n",
        "            im_size = 64\n",
        "        elif dataset == \"celebahq\":\n",
        "            im_size = 128\n",
        "        elif dataset == \"object\":\n",
        "            im_size = 128\n",
        "        elif dataset == \"mnist\":\n",
        "            im_size = 28\n",
        "        elif dataset == \"moving_mnist\":\n",
        "            im_size = 28\n",
        "        elif dataset == \"imagenet\":\n",
        "            im_size = 128\n",
        "        elif dataset == \"lsun\":\n",
        "            im_size = 128\n",
        "        elif dataset == \"stl\":\n",
        "            im_size = 48\n",
        "        else:\n",
        "            assert False\n",
        "\n",
        "        color_transform = get_color_distortion(0.5)\n",
        "        self.dataset = dataset\n",
        "\n",
        "        if transform:\n",
        "            if dataset in (\"cifar10\", \"celeba\", \"cats\"):\n",
        "                color_transform = get_color_distortion(1.0)\n",
        "                self.transform = transforms.Compose([transforms.RandomResizedCrop(im_size, scale=(0.08, 1.0)), transforms.RandomHorizontalFlip(), color_transform, transforms.ToTensor()])\n",
        "                # self.transform = transforms.Compose([transforms.RandomResizedCrop(im_size, scale=(0.03, 1.0)), transforms.RandomHorizontalFlip(), color_transform, GaussianBlur(kernel_size=5), transforms.ToTensor()])\n",
        "            elif dataset == \"continual\":\n",
        "                self.transform = transforms.Compose([transforms.RandomResizedCrop(im_size, scale=(0.08, 1.0)), transforms.RandomHorizontalFlip(), color_transform, GaussianBlur(kernel_size=5), transforms.ToTensor()])\n",
        "            elif dataset == \"celebahq\":\n",
        "                self.transform = transforms.Compose([transforms.RandomResizedCrop(im_size, scale=(0.08, 1.0)), transforms.RandomHorizontalFlip(), color_transform, GaussianBlur(kernel_size=5), transforms.ToTensor()])\n",
        "            elif dataset == \"imagenet\":\n",
        "                self.transform = transforms.Compose([transforms.RandomResizedCrop(im_size, scale=(0.6, 1.0)), transforms.RandomHorizontalFlip(), color_transform, GaussianBlur(kernel_size=11), transforms.ToTensor()])\n",
        "            elif dataset == \"lsun\":\n",
        "                self.transform = transforms.Compose([transforms.RandomResizedCrop(im_size, scale=(0.08, 1.0)), transforms.RandomHorizontalFlip(), color_transform, GaussianBlur(kernel_size=5), transforms.ToTensor()])\n",
        "            elif dataset == \"stl\":\n",
        "                self.transform = transforms.Compose([transforms.RandomResizedCrop(im_size, scale=(0.04, 1.0)), transforms.RandomHorizontalFlip(), color_transform, GaussianBlur(kernel_size=11), transforms.ToTensor()])\n",
        "            elif dataset == \"object\":\n",
        "                self.transform = transforms.Compose([transforms.RandomResizedCrop(im_size, scale=(0.08, 1.0)), transforms.RandomHorizontalFlip(), color_transform, transforms.ToTensor()])\n",
        "            elif dataset == \"mnist\":\n",
        "                self.transform = None\n",
        "            elif dataset == \"moving_mnist\":\n",
        "                self.transform = None\n",
        "            else:\n",
        "                assert False\n",
        "        else:\n",
        "            self.transform = None\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._storage)\n",
        "\n",
        "    def add(self, ims):\n",
        "        batch_size = ims.shape[0]\n",
        "        if self._next_idx >= len(self._storage):\n",
        "            self._storage.extend(list(ims))\n",
        "            self.n = self.n + ims.shape[0]\n",
        "        else:\n",
        "            for im in ims:\n",
        "                self.n = self.n + 1\n",
        "                ix = random.randint(0, self.n - 1)\n",
        "\n",
        "                if ix < len(self._storage):\n",
        "                    self._storage[ix] = im\n",
        "\n",
        "        self._next_idx = (self._next_idx + ims.shape[0]) % self._maxsize\n",
        "\n",
        "\n",
        "    def _encode_sample(self, idxes, no_transform=False, downsample=False):\n",
        "        ims = []\n",
        "        for i in idxes:\n",
        "            im = self._storage[i]\n",
        "\n",
        "            if self.dataset != \"mnist\":\n",
        "                if (self.transform is not None) and (not no_transform):\n",
        "                    im = im.transpose((1, 2, 0))\n",
        "                    im = np.array(self.transform(Image.fromarray(im)))\n",
        "\n",
        "                # if downsample and (self.dataset in [\"celeba\", \"object\", \"imagenet\"]):\n",
        "                #     im = im[:, ::4, ::4]\n",
        "\n",
        "            im = im * 255\n",
        "\n",
        "            ims.append(im)\n",
        "        return np.array(ims)\n",
        "\n",
        "    def sample(self, batch_size, no_transform=False, downsample=False):\n",
        "        \"\"\"Sample a batch of experiences.\n",
        "        Parameters\n",
        "        ----------\n",
        "        batch_size: int\n",
        "            How many transitions to sample.\n",
        "        Returns\n",
        "        -------\n",
        "        obs_batch: np.array\n",
        "            batch of observations\n",
        "        act_batch: np.array\n",
        "            batch of actions executed given obs_batch\n",
        "        rew_batch: np.array\n",
        "            rewards received as results of executing act_batch\n",
        "        next_obs_batch: np.array\n",
        "            next set of observations seen after executing act_batch\n",
        "        done_mask: np.array\n",
        "            done_mask[i] = 1 if executing act_batch[i] resulted in\n",
        "            the end of an episode and 0 otherwise.\n",
        "        \"\"\"\n",
        "        # idxes = [random.randint(0, len(self._storage) - 1)  for _ in range(batch_size)]\n",
        "        idxes = torch.randint(low=0, high=len(self._storage) - 1, size=(batch_size,))\n",
        "        return self._encode_sample(idxes, no_transform=no_transform, downsample=downsample), idxes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5wrgilZg1Xa"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pinBbPL7gMDB"
      },
      "source": [
        "def swish(x):\n",
        "    return x * torch.sigmoid(x)\n",
        "    \n",
        "def adjust_learning_rate(epoch, opt, optimizer):\n",
        "    \"\"\"Sets the learning rate to the initial LR decayed by 0.2 every steep step\"\"\"\n",
        "    steps = np.sum(epoch > np.asarray(opt.lr_decay_epochs))\n",
        "    if steps > 0:\n",
        "        new_lr = opt.learning_rate * (opt.lr_decay_rate ** steps)\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = new_lr\n",
        "\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
        "    with torch.no_grad():\n",
        "        maxk = max(topk)\n",
        "        batch_size = target.size(0)\n",
        "\n",
        "        _, pred = output.topk(maxk, 1, True, True)\n",
        "        pred = pred.t()\n",
        "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "        res = []\n",
        "        for k in topk:\n",
        "            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
        "            res.append(correct_k.mul_(100.0 / batch_size))\n",
        "        return res\n",
        "\n",
        "def compute_jacobian_generic(y, x, create_graph=False):\n",
        "    # Computes the jacobian by tiling values.\n",
        "    # Assumes y is of shape n x d\n",
        "    # Assumes x is of shape n x d also\n",
        "\n",
        "    latent_dim = y.size(1)\n",
        "    grad_y = torch.zeros_like(y)\n",
        "    jacs = []\n",
        "\n",
        "    for i in range(latent_dim):\n",
        "        grad_y[:, i] = 1\n",
        "        jac = torch.autograd.grad(y, x, grad_y, create_graph=create_graph, retain_graph=True)[0]\n",
        "        jacs.append(jac)\n",
        "        grad_y[:, i] = 0\n",
        "\n",
        "    jacs = torch.stack(jacs, dim=1)\n",
        "    return jacs\n",
        "\n",
        "\n",
        "def compute_jacobian(model, im_feat, latent, optimize_partition=False, create_graph=False):\n",
        "    # Computes the jacobian by tiling values.\n",
        "    # Assumes y is of shape n x d\n",
        "    # Assumes x is of shape n x d also\n",
        "    latent_dim = model.energy_dim\n",
        "\n",
        "    im_shape = im_feat.size()\n",
        "    latent_shape = latent.size()\n",
        "    im_feat_raw = im_feat\n",
        "\n",
        "    im_feat = im_feat[:, None, :].repeat(1, latent_dim, 1).view(-1, im_shape[1])\n",
        "    latent = latent[:, None, :].repeat(1, latent_dim, 1).view(-1, latent_shape[1])\n",
        "    grad_y = torch.eye(latent_dim).to(im_feat.device)[None, :, :].repeat(im_shape[0], 1, 1)\n",
        "    grad_y = grad_y.view(-1, latent_dim)\n",
        "    energy = model.feat_energy(im_feat, latent)\n",
        "\n",
        "    if optimize_partition:\n",
        "        im_feat_raw = im_feat_raw[torch.randperm(im_feat_raw.size(0)).to(im_feat_raw.device)][:32]\n",
        "        # im_feat_raw = im_feat_raw\n",
        "        im_feat_partition = im_feat_raw[:, None, :].repeat(1, latent.size(0), 1)\n",
        "        latent_neg_partition = latent[None, :, :].repeat(im_feat_raw.size(0), 1, 1)\n",
        "        partition_est = model.feat_energy(im_feat_partition, latent_neg_partition)\n",
        "        energy = energy + torch.logsumexp(-1 * partition_est, dim=0)\n",
        "\n",
        "    jacs = torch.autograd.grad(energy, latent, grad_y, create_graph=create_graph)[0]\n",
        "    s = jacs.size()\n",
        "    # jacs = jacs.view(im_shape[0], -1)\n",
        "    jacs_dense = jacs.view(im_shape[0], -1)\n",
        "    scale_factor = torch.abs(jacs_dense).max(dim=-1, keepdim=True)[0]\n",
        "\n",
        "    jacs = jacs_dense.view(im_shape[0], -1) / scale_factor\n",
        "    jacs = jacs.view(im_shape[0], latent_dim, s[1])\n",
        "\n",
        "    energy = energy.view(-1, latent_dim, latent_dim)\n",
        "    energy = energy[:, 0, :]\n",
        "\n",
        "    return jacs, scale_factor, energy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACs9FDNCgRzZ"
      },
      "source": [
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eB-bSnuyS9t"
      },
      "source": [
        "class WSConv2d(nn.Conv2d):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n",
        "                 padding=0, dilation=1, groups=1, bias=True):\n",
        "        super(WSConv2d, self).__init__(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        weight = self.weight\n",
        "        weight_mean = weight.mean(dim=1, keepdim=True).mean(dim=2,\n",
        "                                  keepdim=True).mean(dim=3, keepdim=True)\n",
        "        weight = weight - weight_mean\n",
        "        std = weight.view(weight.size(0), -1).std(dim=1).view(-1, 1, 1, 1) + 1e-5\n",
        "        weight = weight / std.expand_as(weight)\n",
        "        return F.conv2d(x, weight, self.bias, self.stride,\n",
        "                        self.padding, self.dilation, self.groups)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bD0tTfB0yeID"
      },
      "source": [
        "class GaussianSmoothing(nn.Module):\n",
        "    \"\"\"\n",
        "    Apply gaussian smoothing on a\n",
        "    1d, 2d or 3d tensor. Filtering is performed seperately for each channel\n",
        "    in the input using a depthwise convolution.\n",
        "    Arguments:\n",
        "        channels (int, sequence): Number of channels of the input tensors. Output will\n",
        "            have this number of channels as well.\n",
        "        kernel_size (int, sequence): Size of the gaussian kernel.\n",
        "        sigma (float, sequence): Standard deviation of the gaussian kernel.\n",
        "        dim (int, optional): The number of dimensions of the data.\n",
        "            Default value is 2 (spatial).\n",
        "    \"\"\"\n",
        "    def __init__(self, channels, kernel_size, sigma, dim=2):\n",
        "        super(GaussianSmoothing, self).__init__()\n",
        "        if isinstance(kernel_size, numbers.Number):\n",
        "            kernel_size = [kernel_size] * dim\n",
        "        if isinstance(sigma, numbers.Number):\n",
        "            sigma = [sigma] * dim\n",
        "\n",
        "        # The gaussian kernel is the product of the\n",
        "        # gaussian function of each dimension.\n",
        "        kernel = 1\n",
        "        meshgrids = torch.meshgrid(\n",
        "            [\n",
        "                torch.arange(size, dtype=torch.float32)\n",
        "                for size in kernel_size\n",
        "            ]\n",
        "        )\n",
        "        for size, std, mgrid in zip(kernel_size, sigma, meshgrids):\n",
        "            mean = (size - 1) / 2\n",
        "            kernel *= 1 / (std * math.sqrt(2 * math.pi)) * \\\n",
        "                      torch.exp(-((mgrid - mean) / std) ** 2 / 2)\n",
        "\n",
        "        # Make sure sum of values in gaussian kernel equals 1.\n",
        "        kernel = kernel / torch.sum(kernel)\n",
        "\n",
        "        # Reshape to depthwise convolutional weight\n",
        "        kernel = kernel.view(1, 1, *kernel.size())\n",
        "        kernel = kernel.repeat(channels, *[1] * (kernel.dim() - 1))\n",
        "\n",
        "        self.register_buffer('weight', kernel)\n",
        "        self.groups = channels\n",
        "\n",
        "        if dim == 1:\n",
        "            self.conv = F.conv1d\n",
        "        elif dim == 2:\n",
        "            self.conv = F.conv2d\n",
        "        elif dim == 3:\n",
        "            self.conv = F.conv3d\n",
        "        else:\n",
        "            raise RuntimeError(\n",
        "                'Only 1, 2 and 3 dimensions are supported. Received {}.'.format(dim)\n",
        "            )\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        Apply gaussian filter to input.\n",
        "        Arguments:\n",
        "            input (torch.Tensor): Input to apply gaussian filter on.\n",
        "        Returns:\n",
        "            filtered (torch.Tensor): Filtered output.\n",
        "        \"\"\"\n",
        "        return self.conv(input, weight=self.weight, groups=self.groups)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxATt62jy_rD"
      },
      "source": [
        "def cutout(mask_color=(0, 0, 0)):\n",
        "    mask_size_half = flags_cutout_mask_size // 2\n",
        "    offset = 1 if flags_cutout_mask_size % 2 == 0 else 0\n",
        "\n",
        "    def _cutout(image):\n",
        "        image = np.asarray(image).copy()\n",
        "\n",
        "        if np.random.random() > flags_cutout_prob:\n",
        "            return image\n",
        "\n",
        "        h, w = image.shape[:2]\n",
        "\n",
        "        if flags_cutout_inside:\n",
        "            cxmin, cxmax = mask_size_half, w + offset - mask_size_half\n",
        "            cymin, cymax = mask_size_half, h + offset - mask_size_half\n",
        "        else:\n",
        "            cxmin, cxmax = 0, w + offset\n",
        "            cymin, cymax = 0, h + offset\n",
        "\n",
        "        cx = np.random.randint(cxmin, cxmax)\n",
        "        cy = np.random.randint(cymin, cymax)\n",
        "        xmin = cx - mask_size_half\n",
        "        ymin = cy - mask_size_half\n",
        "        xmax = xmin + flags_cutout_mask_size\n",
        "        ymax = ymin + flags_cutout_mask_size\n",
        "        xmin = max(0, xmin)\n",
        "        ymin = max(0, ymin)\n",
        "        xmax = min(w, xmax)\n",
        "        ymax = min(h, ymax)\n",
        "        image[:, ymin:ymax, xmin:xmax] = np.array(mask_color)[:, None, None]\n",
        "        return image\n",
        "\n",
        "    return _cutout"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QyNlhx8zJrw"
      },
      "source": [
        "def compress_x_mod(x_mod):\n",
        "    x_mod = (255 * np.clip(x_mod, 0, 1)).astype(np.uint8)\n",
        "    return x_mod\n",
        "\n",
        "\n",
        "def decompress_x_mod(x_mod):\n",
        "    x_mod = x_mod / 256  + \\\n",
        "        np.random.uniform(0, 1 / 256, x_mod.shape)\n",
        "    return x_mod"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgQCQlI_zOcX"
      },
      "source": [
        "def sync_model(models):\n",
        "    size = float(dist.get_world_size())\n",
        "\n",
        "    for model in models:\n",
        "        for param in model.parameters():\n",
        "            dist.broadcast(param.data, 0)\n",
        "\n",
        "\n",
        "def ema_model(models, models_ema, mu=0.99):\n",
        "    for model, model_ema in zip(models, models_ema):\n",
        "        for param, param_ema in zip(model.parameters(), model_ema.parameters()):\n",
        "            param_ema.data[:] = mu * param_ema.data + (1 - mu) * param.data\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06rHTEb4zQBd"
      },
      "source": [
        "def average_gradients(models):\n",
        "    size = float(dist.get_world_size())\n",
        "\n",
        "    for model in models:\n",
        "        for param in model.parameters():\n",
        "            if param.grad is None:\n",
        "                continue\n",
        "\n",
        "            dist.all_reduce(param.grad.data, op=dist.reduce_op.SUM)\n",
        "            param.grad.data /= size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzYJvCPFhE9h"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVD8UCQ-zHlh"
      },
      "source": [
        "class Mnist(Dataset):\n",
        "    def __init__(self, train=True, rescale=1.0):\n",
        "        self.data = MNIST(\n",
        "            \"data/mnist\",\n",
        "            transform=transforms.ToTensor(),\n",
        "            download=True, train=train)\n",
        "        self.labels = np.eye(10)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        im, label = self.data[index]\n",
        "        label = self.labels[label]\n",
        "        im = im.squeeze()\n",
        "        im = im.numpy() / 256 * 255 + np.random.uniform(0, 1. / 256, (28, 28))\n",
        "        im = np.clip(im, 0, 1)\n",
        "        s = 28\n",
        "        im_corrupt = np.random.uniform(0, 1, (s, s, 1))\n",
        "        im = im[:, :, None]\n",
        "\n",
        "        return torch.Tensor(im_corrupt), torch.Tensor(im), label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmB6PrzwOKMG"
      },
      "source": [
        "if flags.dataset == \"celebahq\":\n",
        "    !mkdir -p /content/data/celebAHQ\n",
        "    !unzip -qq '/content/drive/MyDrive/Colab Notebooks/improved_contrastive_divergence/data/celebAHQ/data128x128.zip' -d /content/data/celebAHQ\n",
        "elif flags.dataset == \"celeba\":\n",
        "    !mkdir -p /content/data\n",
        "    %cd /content/drive/MyDrive/Colab Notebooks/improved_contrastive_divergence\n",
        "    %cp -av data/celeba/ /content/data\n",
        "elif flags.dataset == \"cats\":\n",
        "    !mkdir -p /content/data\n",
        "    %cd /content/drive/MyDrive/Colab Notebooks/improved_contrastive_divergence\n",
        "    %cp -av data/cats/ /content/data\n",
        "    !unzip -qq /content/data/cats/cats-dataset.zip -d /content/data/cats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPUQRGHhqcey"
      },
      "source": [
        " class CelebAHQ(Dataset):\n",
        "\n",
        "    def __init__(self, cond_idx=1, filter_idx=0):\n",
        "        self.path = \"/content/data/celebAHQ/data128x128/{:05}.jpg\"\n",
        "        self.hq_labels = pd.read_csv(os.path.join(sample_dir, \"data/celebAHQ/image_list.txt\"), sep=\"\\s+\")\n",
        "        self.labels = pd.read_csv(os.path.join(sample_dir, \"data/celebAHQ/list_attr_celeba.txt\"), sep=\"\\s+\", skiprows=1)\n",
        "        self.cond_idx = cond_idx\n",
        "        self.filter_idx = filter_idx\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.hq_labels.shape[0] \n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        info = self.hq_labels.iloc[index]\n",
        "        info = self.labels.iloc[info.orig_idx]\n",
        "\n",
        "        path = self.path.format(index+1)\n",
        "        im = np.array(Image.open(path))\n",
        "        image_size = 128\n",
        "        # im = imresize(im, (image_size, image_size))\n",
        "        im = im / 256\n",
        "        im = im + np.random.uniform(0, 1 / 256., im.shape)\n",
        "\n",
        "        label = int(info.iloc[self.cond_idx])\n",
        "        if label == -1:\n",
        "            label = 0\n",
        "        label = np.eye(2)[label]\n",
        "\n",
        "        im_corrupt = np.random.uniform(\n",
        "            0, 1, size=(image_size, image_size, 3))\n",
        "\n",
        "        return im_corrupt, im, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7D46zFv55qZ5"
      },
      "source": [
        "class CelebADataset(Dataset):\n",
        "    def __init__(\n",
        "            self,\n",
        "            FLAGS,\n",
        "            split='train',\n",
        "            augment=False,\n",
        "            noise=True,\n",
        "            rescale=1.0):\n",
        "\n",
        "        if augment:\n",
        "            transform_list = [\n",
        "                torchvision.transforms.RandomCrop(32, padding=4),\n",
        "                torchvision.transforms.RandomHorizontalFlip(),\n",
        "                torchvision.transforms.ToTensor(),\n",
        "            ]\n",
        "\n",
        "            transform = transforms.Compose(transform_list)\n",
        "        else:\n",
        "            # transform = transforms.ToTensor()\n",
        "            transform = transforms.Compose([\n",
        "                # resize\n",
        "                transforms.Resize(32),\n",
        "                # center-crop\n",
        "                transforms.CenterCrop(32),\n",
        "                # to-tensor\n",
        "                transforms.ToTensor()\n",
        "            ])\n",
        "\n",
        "        self.data = torchvision.datasets.CelebA(\n",
        "            \"/content/data\",\n",
        "            transform=transform,\n",
        "            split=split,\n",
        "            download=True)\n",
        "        self.one_hot_map = np.eye(10)\n",
        "        self.noise = noise\n",
        "        self.rescale = rescale\n",
        "        self.FLAGS = FLAGS\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        FLAGS = self.FLAGS\n",
        "        \n",
        "        im, label = self.data[index]\n",
        "\n",
        "        im = np.transpose(im, (1, 2, 0)).numpy()\n",
        "        image_size = 32\n",
        "        label = self.one_hot_map[label]\n",
        "\n",
        "        im = im * 255 / 256\n",
        "\n",
        "        im = im * self.rescale + \\\n",
        "            np.random.uniform(0, 1 / 256., im.shape)\n",
        "\n",
        "        # np.random.seed((index + int(time.time() * 1e7)) % 2**32)\n",
        "\n",
        "        im_corrupt = np.random.uniform(\n",
        "            0.0, self.rescale, (image_size, image_size, 3))\n",
        "\n",
        "        return torch.Tensor(im_corrupt), torch.Tensor(im), label\n",
        "        # return torch.Tensor(im), label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Inp98HlwjB6"
      },
      "source": [
        "class Cats(Dataset):\n",
        "    def __init__(\n",
        "            self,\n",
        "            augment=False,\n",
        "            noise=True,\n",
        "            rescale=1.0):\n",
        "\n",
        "        if augment:\n",
        "            transform_list = [\n",
        "                torchvision.transforms.RandomCrop(32, padding=4),\n",
        "                torchvision.transforms.RandomHorizontalFlip(),\n",
        "                torchvision.transforms.ToTensor(),\n",
        "            ]\n",
        "\n",
        "            transform = transforms.Compose(transform_list)\n",
        "        else:\n",
        "            # transform = transforms.ToTensor()\n",
        "            transform = transforms.Compose([\n",
        "                # resize\n",
        "                transforms.Resize(32),\n",
        "                # center-crop\n",
        "                transforms.CenterCrop(32),\n",
        "                # to-tensor\n",
        "                transforms.ToTensor()\n",
        "            ])\n",
        "\n",
        "        self.data = torchvision.datasets.ImageFolder('/content/data/cats', transform = transform)\n",
        "        self.one_hot_map = np.eye(10)\n",
        "        self.noise = noise\n",
        "        self.rescale = rescale\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):        \n",
        "        im, label = self.data[index]\n",
        "\n",
        "        im = np.transpose(im, (1, 2, 0)).numpy()\n",
        "        image_size = 32\n",
        "        label = self.one_hot_map[label]\n",
        "\n",
        "        im = im * 255 / 256\n",
        "\n",
        "        im = im * self.rescale + \\\n",
        "            np.random.uniform(0, 1 / 256., im.shape)\n",
        "\n",
        "        im_corrupt = np.random.uniform(\n",
        "            0.0, self.rescale, (image_size, image_size, 3))\n",
        "\n",
        "        return torch.Tensor(im_corrupt), torch.Tensor(im), label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfhRYDURAlRC"
      },
      "source": [
        "class Cifar10(Dataset):\n",
        "    def __init__(\n",
        "            self,\n",
        "            FLAGS,\n",
        "            train=True,\n",
        "            full=False,\n",
        "            augment=False,\n",
        "            noise=True,\n",
        "            rescale=1.0):\n",
        "\n",
        "        if augment:\n",
        "            transform_list = [\n",
        "                torchvision.transforms.RandomCrop(32, padding=4),\n",
        "                torchvision.transforms.RandomHorizontalFlip(),\n",
        "                torchvision.transforms.ToTensor(),\n",
        "            ]\n",
        "\n",
        "            transform = transforms.Compose(transform_list)\n",
        "        else:\n",
        "            transform = transforms.ToTensor()\n",
        "\n",
        "        self.full = full\n",
        "        self.data = torchvision.datasets.CIFAR10(\n",
        "            \"./data/cifar10\",\n",
        "            transform=transform,\n",
        "            train=train,\n",
        "            download=True)\n",
        "        self.test_data = torchvision.datasets.CIFAR10(\n",
        "            \"./data/cifar10\",\n",
        "            transform=transform,\n",
        "            train=False,\n",
        "            download=True)\n",
        "        self.one_hot_map = np.eye(10)\n",
        "        self.noise = noise\n",
        "        self.rescale = rescale\n",
        "        self.FLAGS = FLAGS\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        if self.full:\n",
        "            return len(self.data) + len(self.test_data)\n",
        "        else:\n",
        "            return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        FLAGS = self.FLAGS\n",
        "        if self.full:\n",
        "            if index >= len(self.data):\n",
        "                im, label = self.test_data[index - len(self.data)]\n",
        "            else:\n",
        "                im, label = self.data[index]\n",
        "        else:\n",
        "            im, label = self.data[index]\n",
        "\n",
        "        im = np.transpose(im, (1, 2, 0)).numpy()\n",
        "        image_size = 32\n",
        "        label = self.one_hot_map[label]\n",
        "\n",
        "        im = im * 255 / 256\n",
        "\n",
        "        im = im * self.rescale + \\\n",
        "            np.random.uniform(0, 1 / 256., im.shape)\n",
        "\n",
        "        # np.random.seed((index + int(time.time() * 1e7)) % 2**32)\n",
        "\n",
        "        im_corrupt = np.random.uniform(\n",
        "            0.0, self.rescale, (image_size, image_size, 3))\n",
        "\n",
        "        return torch.Tensor(im_corrupt), torch.Tensor(im), label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJaPpb3VjZkw"
      },
      "source": [
        "## Sampling ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Zs3jBihzWtN"
      },
      "source": [
        "def rescale_im(image):\n",
        "    image = np.clip(image, 0, 1)\n",
        "    return (np.clip(image * 256, 0, 255)).astype(np.uint8)\n",
        "\n",
        "\n",
        "def hamiltonian(x, v, model, label):\n",
        "    energy = 0.5 * torch.pow(v, 2).sum(dim=1).sum(dim=1).sum(dim=1) + model.forward(x, label).squeeze()\n",
        "    return energy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xLaBQUVzcVE"
      },
      "source": [
        "def leapfrog_step(x, v, model, step_size, num_steps, label, sample=False):\n",
        "    x.requires_grad_(requires_grad=True)\n",
        "    energy = model.forward(x, label)\n",
        "    im_grad = torch.autograd.grad([energy.sum()], [x])[0]\n",
        "    v = v - 0.5 * step_size * im_grad\n",
        "    im_negs = []\n",
        "\n",
        "    for i in range(num_steps):\n",
        "        x.requires_grad_(requires_grad=True)\n",
        "        energy = model.forward(x, label)\n",
        "\n",
        "        if i == num_steps - 1:\n",
        "            im_grad = torch.autograd.grad([energy.sum()], [x], create_graph=True)[0]\n",
        "            v = v - step_size * im_grad\n",
        "            x = x + step_size * v\n",
        "            v = v.detach()\n",
        "        else:\n",
        "            im_grad = torch.autograd.grad([energy.sum()], [x])[0]\n",
        "            v = v - step_size * im_grad\n",
        "            x = x + step_size * v\n",
        "            x = x.detach()\n",
        "            v = v.detach()\n",
        "\n",
        "\n",
        "        if sample:\n",
        "            im_negs.append(x)\n",
        "\n",
        "        if i % 10 == 0:\n",
        "            print(i, hamiltonian(torch.sigmoid(x), v, model, label).mean(), torch.abs(im_grad).mean())\n",
        "\n",
        "    if sample:\n",
        "        return x, im_negs, v, im_grad\n",
        "    else:\n",
        "        return x, v, im_grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiiP4nJnzejO"
      },
      "source": [
        "def gen_hmc_image(label, FLAGS, model, im_neg, num_steps, sample=False):\n",
        "    step_size = FLAGS.step_lr\n",
        "\n",
        "    v = 0.001 * torch.randn_like(im_neg)\n",
        "\n",
        "    if sample:\n",
        "        im_neg, im_negs, v, im_grad = leapfrog_step(im_neg, v, model, step_size, num_steps, label, sample=sample)\n",
        "        return im_neg, im_negs, im_grad, v\n",
        "    else:\n",
        "        im_neg, v, im_grad = leapfrog_step(im_neg, v, model, step_size, num_steps, label, sample=sample)\n",
        "        return im_neg, im_grad, v"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIYEMoQmzfmR"
      },
      "source": [
        "def gen_image(label, FLAGS, model, im_neg, num_steps, sample=False):\n",
        "    im_noise = torch.randn_like(im_neg).detach()\n",
        "\n",
        "    im_negs_samples = []\n",
        "\n",
        "    for i in range(num_steps):\n",
        "        im_noise.normal_()\n",
        "\n",
        "        if FLAGS.anneal:\n",
        "            im_neg = im_neg + 0.001 * (num_steps - i - 1) / num_steps * im_noise\n",
        "        else:\n",
        "            im_neg = im_neg + 0.001 * im_noise\n",
        "\n",
        "        im_neg.requires_grad_(requires_grad=True)\n",
        "        energy = model.forward(im_neg, label)\n",
        "\n",
        "        if FLAGS.all_step:\n",
        "            im_grad = torch.autograd.grad([energy.sum()], [im_neg], create_graph=True)[0]\n",
        "        else:\n",
        "            im_grad = torch.autograd.grad([energy.sum()], [im_neg])[0]\n",
        "\n",
        "        if i == num_steps - 1:\n",
        "            im_neg_orig = im_neg\n",
        "            im_neg = im_neg - FLAGS.step_lr * im_grad\n",
        "\n",
        "            if FLAGS.dataset in (\"cifar10\", \"celeba\", \"cats\"):\n",
        "                n = 128\n",
        "            elif FLAGS.dataset == \"celebahq\":\n",
        "                # Save space\n",
        "                n = 128\n",
        "            elif FLAGS.dataset == \"lsun\":\n",
        "                # Save space\n",
        "                n = 32\n",
        "            elif FLAGS.dataset == \"object\":\n",
        "                # Save space\n",
        "                n = 32\n",
        "            elif FLAGS.dataset == \"mnist\":\n",
        "                n = 32\n",
        "            elif FLAGS.dataset == \"imagenet\":\n",
        "                n = 32\n",
        "            elif FLAGS.dataset == \"stl\":\n",
        "                n = 32\n",
        "\n",
        "            im_neg_kl = im_neg_orig[:n]\n",
        "            if sample:\n",
        "                pass\n",
        "            else:\n",
        "                energy = model.forward(im_neg_kl, label)\n",
        "                im_grad = torch.autograd.grad([energy.sum()], [im_neg_kl], create_graph=True)[0]\n",
        "\n",
        "            im_neg_kl = im_neg_kl - FLAGS.step_lr * im_grad[:n]\n",
        "            im_neg_kl = torch.clamp(im_neg_kl, 0, 1)\n",
        "        else:\n",
        "            im_neg = im_neg - FLAGS.step_lr * im_grad\n",
        "\n",
        "        im_neg = im_neg.detach()\n",
        "\n",
        "        if sample:\n",
        "            im_negs_samples.append(im_neg)\n",
        "\n",
        "        im_neg = torch.clamp(im_neg, 0, 1)\n",
        "\n",
        "    if sample:\n",
        "        return im_neg, im_neg_kl, im_negs_samples, im_grad\n",
        "    else:\n",
        "        return im_neg, im_neg_kl, im_grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ThYxXgAHnpy"
      },
      "source": [
        "def gen_image_asgld(label, FLAGS, model, im_neg, num_steps, sample=False):\n",
        "    # im_noise = torch.randn_like(im_neg).detach()\n",
        "\n",
        "    im_negs_samples = []\n",
        "    \n",
        "    # Intialize mean and variance to zero\n",
        "    mean = torch.zeros_like(im_neg.data)\n",
        "    std = torch.zeros_like(im_neg.data)\n",
        "    weight_decay = 5e-4\n",
        "\n",
        "    for i in range(num_steps):\n",
        "        # im_noise.normal_()\n",
        "        # Getting mean,std at previous step\n",
        "        old_mean = mean.clone()\n",
        "        old_std = std.clone()\n",
        "\n",
        "        im_noise = torch.normal(mean=old_mean, std=old_std)\n",
        "        # im_neg.data.add(FLAGS.init_noise, noise)\n",
        "\n",
        "        if FLAGS.anneal:\n",
        "            # im_neg = im_neg + 0.001 * (num_steps - i - 1) / num_steps * im_noise\n",
        "            im_neg = im_neg + FLAGS.init_noise * (num_steps - i - 1) / num_steps * im_noise\n",
        "        else:\n",
        "            # im_neg = im_neg + 0.001 * im_noise\n",
        "            im_neg.data.add(FLAGS.init_noise, im_noise)\n",
        "\n",
        "        im_neg.requires_grad_(requires_grad=True)\n",
        "        energy = model.forward(im_neg, label)\n",
        "\n",
        "        if FLAGS.all_step:\n",
        "            im_grad = torch.autograd.grad([energy.sum()], [im_neg], create_graph=True)[0]\n",
        "        else:\n",
        "            im_grad = torch.autograd.grad([energy.sum()], [im_neg])[0]\n",
        "\n",
        "        if i == num_steps - 1:\n",
        "            im_neg_orig = im_neg\n",
        "            im_neg = im_neg - FLAGS.step_lr * im_grad\n",
        "\n",
        "            if FLAGS.dataset in (\"cifar10\", \"celeba\", \"cats\"):\n",
        "                n = 128\n",
        "            elif FLAGS.dataset == \"celebahq\":\n",
        "                # Save space\n",
        "                n = 128\n",
        "            elif FLAGS.dataset == \"lsun\":\n",
        "                # Save space\n",
        "                n = 32\n",
        "            elif FLAGS.dataset == \"object\":\n",
        "                # Save space\n",
        "                n = 32\n",
        "            elif FLAGS.dataset == \"mnist\":\n",
        "                n = 32\n",
        "            elif FLAGS.dataset == \"imagenet\":\n",
        "                n = 32\n",
        "            elif FLAGS.dataset == \"stl\":\n",
        "                n = 32\n",
        "\n",
        "            im_neg_kl = im_neg_orig[:n]\n",
        "            if sample:\n",
        "                pass\n",
        "            else:\n",
        "                energy = model.forward(im_neg_kl, label)\n",
        "                im_grad = torch.autograd.grad([energy.sum()], [im_neg_kl], create_graph=True)[0]\n",
        "\n",
        "            im_neg_kl = im_neg_kl - FLAGS.step_lr * im_grad[:n]\n",
        "            im_neg_kl = torch.clamp(im_neg_kl, 0, 1)\n",
        "            im_neg_kl = im_neg_kl.detach()\n",
        "        else:\n",
        "            im_neg = im_neg - FLAGS.step_lr * im_grad        \n",
        "\n",
        "        # Updating mean\n",
        "        mean = mean.mul(FLAGS.momentum).add(im_neg.data)\n",
        "            \n",
        "        # Updating std\n",
        "        part_var1 = im_neg.data.add(-old_mean)\n",
        "        part_var2 = im_neg.data.add(-mean)\n",
        "            \n",
        "        new_std = torch.pow(old_std,2).mul(FLAGS.momentum).addcmul(1,part_var1,part_var2).add(FLAGS.eps)                \n",
        "        new_std = torch.pow(torch.abs_(new_std),1/2)\n",
        "        std.add_(-1,std).add_(new_std)\n",
        "            \n",
        "        im_neg = im_neg.detach()\n",
        "\n",
        "        if sample:\n",
        "            im_negs_samples.append(im_neg)\n",
        "\n",
        "        im_neg = torch.clamp(im_neg, 0, 1)\n",
        "    \n",
        "    if sample:\n",
        "        return im_neg, im_neg_kl, im_negs_samples, np.abs(im_grad.detach().cpu().numpy()).mean()\n",
        "    else:\n",
        "        return im_neg, im_neg_kl, np.abs(im_grad.detach().cpu().numpy()).mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZzpwMtXnCUi"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-77C3ocGz-wG"
      },
      "source": [
        "def test(model, logger, dataloader):\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvOiNixGFlV4"
      },
      "source": [
        "def log_tensorboard(data):\n",
        "    writer.add_scalar(\"repel loss\", data[\"loss_repel\"], data[\"iter\"])\n",
        "    writer.add_scalar(\"KL mean\", data[\"kl_mean\"], data[\"iter\"])\n",
        "    \n",
        "\n",
        "    writer.add_scalar(\"positive energy mean\", data[\"e_pos\"], data[\"iter\"])\n",
        "    writer.add_scalar(\"positive energy std\", data[\"e_pos_std\"], data[\"iter\"])\n",
        "\n",
        "    writer.add_scalar(\"negative energy mean\", data[\"e_neg\"], data[\"iter\"])\n",
        "    writer.add_scalar(\"negative energy std\", data[\"e_neg_std\"], data[\"iter\"])\n",
        "\n",
        "    writer.add_scalar(\"energy different\", data[\"e_diff\"], data[\"iter\"])\n",
        "    writer.add_scalar(\"x gradient\", data[\"x_grad\"], data[\"iter\"])\n",
        "\n",
        "    writer.add_images(\"positive examples\", data[\"positive_samples\"], data[\"iter\"])\n",
        "    writer.add_images(\"negative examples\", data[\"negative_samples\"], data[\"iter\"])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7RcP5Je0Ag4"
      },
      "source": [
        "def train(model, optimizer, dataloader,logdir, resume_iter, FLAGS, best_inception):\n",
        "    if FLAGS.cuda:\n",
        "        model = model.to(FLAGS.gpu)\n",
        "\n",
        "    if FLAGS.replay_batch:\n",
        "        if FLAGS.reservoir:\n",
        "            replay_buffer = ReservoirBuffer(FLAGS.buffer_size, FLAGS.transform, FLAGS.dataset)\n",
        "        else:\n",
        "            replay_buffer = ReplayBuffer(FLAGS.buffer_size, FLAGS.transform, FLAGS.dataset)\n",
        "\n",
        "    itr = resume_iter\n",
        "    im_neg = None\n",
        "    gd_steps = 1\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    num_steps = FLAGS.num_steps\n",
        "\n",
        "    for epoch in range(FLAGS.epoch_num):\n",
        "        print(\"epoch : \", epoch)\n",
        "        tock = time.time()\n",
        "        for data_corrupt, data, label in dataloader:\n",
        "            # torch.cuda.empty_cache()\n",
        "            label = label.float()\n",
        "            data = data.permute(0, 3, 1, 2).float().contiguous()\n",
        "            \n",
        "            # Generate samples to evaluate inception score\n",
        "            if itr % FLAGS.save_interval == 0:\n",
        "                # print(\"masuk situ diiterasi\", itr)\n",
        "                if FLAGS.dataset in (\"cifar10\", \"celeba\", \"cats\"):\n",
        "                    data_corrupt = torch.Tensor(np.random.uniform(0.0, 1.0, (128, 32, 32, 3)))\n",
        "                    repeat = 128 // FLAGS.batch_size + 1\n",
        "                    label = torch.cat([label] * repeat, axis=0)\n",
        "                    label = label[:128]\n",
        "                elif FLAGS.dataset == \"celebahq\":\n",
        "                    data_corrupt = torch.Tensor(np.random.uniform(0.0, 1.0, (data.shape[0], 128, 128, 3)))\n",
        "                    label = label[:data.shape[0]]\n",
        "                    data_corrupt = data_corrupt[:label.shape[0]]\n",
        "                elif FLAGS.dataset == \"stl\":\n",
        "                    data_corrupt = torch.Tensor(np.random.uniform(0.0, 1.0, (32, 48, 48, 3)))\n",
        "                    label = label[:32]\n",
        "                    data_corrupt = data_corrupt[:label.shape[0]]\n",
        "                elif FLAGS.dataset == \"lsun\":\n",
        "                    data_corrupt = torch.Tensor(np.random.uniform(0.0, 1.0, (32, 128, 128, 3)))\n",
        "                    label = label[:32]\n",
        "                    data_corrupt = data_corrupt[:label.shape[0]]\n",
        "                elif FLAGS.dataset == \"imagenet\":\n",
        "                    data_corrupt = torch.Tensor(np.random.uniform(0.0, 1.0, (32, 128, 128, 3)))\n",
        "                    label = label[:32]\n",
        "                    data_corrupt = data_corrupt[:label.shape[0]]\n",
        "                elif FLAGS.dataset == \"object\":\n",
        "                    data_corrupt = torch.Tensor(np.random.uniform(0.0, 1.0, (32, 128, 128, 3)))\n",
        "                    label = label[:32]\n",
        "                    data_corrupt = data_corrupt[:label.shape[0]]\n",
        "                elif FLAGS.dataset == \"mnist\":\n",
        "                    data_corrupt = torch.Tensor(np.random.uniform(0.0, 1.0, (32, 28, 28, 1)))\n",
        "                    label = label[:32]\n",
        "                    data_corrupt = data_corrupt[:label.shape[0]]\n",
        "                else:\n",
        "                    assert False\n",
        "            \n",
        "            data_corrupt = torch.Tensor(data_corrupt.float()).permute(0, 3, 1, 2).float().contiguous()\n",
        "            \n",
        "            if FLAGS.replay_batch and len(replay_buffer) >= FLAGS.batch_size:\n",
        "                replay_batch, idxs = replay_buffer.sample(data_corrupt.size(0))\n",
        "                replay_batch = decompress_x_mod(replay_batch)\n",
        "                replay_mask = (\n",
        "                    np.random.uniform(\n",
        "                        0,\n",
        "                        1,\n",
        "                        data_corrupt.size(0)) > 0.05)\n",
        "                data_corrupt[replay_mask] = torch.Tensor(replay_batch[replay_mask])\n",
        "            else:\n",
        "                idxs = None\n",
        "\n",
        "            # print(\"data shape\", data.shape)\n",
        "            data = data.to(FLAGS.gpu, non_blocking=True)\n",
        "            data_corrupt = data_corrupt.to(FLAGS.gpu, non_blocking=True)\n",
        "            label = label.to(FLAGS.gpu, non_blocking=True)\n",
        "            \n",
        "            if FLAGS.hmc:\n",
        "                if itr % FLAGS.save_interval == 0:\n",
        "                    im_neg, im_samples, x_grad, v = gen_hmc_image(label, FLAGS, model, data_corrupt, num_steps, sample=True)\n",
        "                else:\n",
        "                    im_neg, x_grad, v = gen_hmc_image(label, FLAGS, model, data_corrupt, num_steps)\n",
        "            elif FLAGS.asgld:\n",
        "                if itr % FLAGS.save_interval == 0:\n",
        "                    im_neg, im_neg_kl, im_samples, x_grad = gen_image_asgld(label, FLAGS, model, data_corrupt, num_steps, sample=True)\n",
        "                else:\n",
        "                    im_neg, im_neg_kl, x_grad = gen_image_asgld(label, FLAGS, model, data_corrupt, num_steps)\n",
        "            else:\n",
        "                if itr % FLAGS.save_interval == 0:\n",
        "                    im_neg, im_neg_kl, im_samples, x_grad = gen_image(label, FLAGS, model, data_corrupt, num_steps, sample=True)\n",
        "                else:\n",
        "                    im_neg, im_neg_kl, x_grad = gen_image(label, FLAGS, model, data_corrupt, num_steps)\n",
        "            \n",
        "\n",
        "            energy_neg = model.forward(im_neg, label)\n",
        "            energy_pos = model.forward(data, label[:data.size(0)])\n",
        "            \n",
        "            if FLAGS.replay_batch and (im_neg is not None):\n",
        "                replay_buffer.add(compress_x_mod(im_neg.detach().cpu().numpy()))\n",
        "\n",
        "            loss = energy_pos.mean() - energy_neg.mean() #\n",
        "            loss = loss  + (torch.pow(energy_pos, 2).mean() + torch.pow(energy_neg, 2).mean())\n",
        "\n",
        "            # print(\"debug loss calculation\", torch.cuda.list_gpu_processes())\n",
        "            # print(\"calculate KL\")\n",
        "            if FLAGS.kl:\n",
        "                model.requires_grad_(False)\n",
        "                loss_kl = model.forward(im_neg_kl, label)\n",
        "                model.requires_grad_(True)\n",
        "                loss = loss + FLAGS.kl_coeff * loss_kl.mean()\n",
        "\n",
        "                if FLAGS.repel_im:\n",
        "                    start = timeit.timeit()\n",
        "                    bs = im_neg_kl.size(0)\n",
        "\n",
        "                    if FLAGS.dataset in [\"celebahq\", \"imagenet\", \"object\", \"lsun\", \"stl\"]:\n",
        "                        im_neg_kl = im_neg_kl[:, :, :, :].contiguous()\n",
        "\n",
        "                    im_flat = torch.clamp(im_neg_kl.view(bs, -1), 0, 1)\n",
        "\n",
        "                    if FLAGS.dataset in (\"cifar10\", \"celeba\", \"cats\"):\n",
        "                        if len(replay_buffer) > 1000:\n",
        "                            compare_batch, idxs = replay_buffer.sample(100, no_transform=False)\n",
        "                            compare_batch = decompress_x_mod(compare_batch)\n",
        "                            compare_batch = torch.Tensor(compare_batch).to(FLAGS.gpu, non_blocking=True)\n",
        "                            # compare_batch = torch.Tensor(compare_batch)\n",
        "                            compare_flat = compare_batch.view(100, -1)\n",
        "\n",
        "                            dist_matrix = torch.norm(im_flat[:, None, :] - compare_flat[None, :, :], p=2, dim=-1)\n",
        "                            loss_repel = torch.log(dist_matrix.min(dim=1)[0]).mean()\n",
        "                            loss = loss - 0.3 * loss_repel\n",
        "                        else:\n",
        "                            loss_repel = torch.zeros(1)\n",
        "                    else:\n",
        "                        if len(replay_buffer) > 1000:\n",
        "                            compare_batch, idxs = replay_buffer.sample(100, no_transform=False, downsample=True)\n",
        "                            compare_batch = decompress_x_mod(compare_batch)\n",
        "                            compare_batch = torch.Tensor(compare_batch).to(FLAGS.gpu, non_blocking=True)\n",
        "                            # compare_batch = torch.Tensor(compare_batch)\n",
        "                            compare_flat = compare_batch.view(100, -1)\n",
        "                            dist_matrix = torch.norm(im_flat[:, None, :] - compare_flat[None, :, :], p=2, dim=-1)\n",
        "                            loss_repel = torch.log(dist_matrix.min(dim=1)[0]).mean()\n",
        "                        else:\n",
        "                            loss_repel = torch.zeros(1).to(FLAGS.gpu, non_blocking=True)\n",
        "\n",
        "                        loss = loss - 0.3 * loss_repel\n",
        "\n",
        "                    end = timeit.timeit()\n",
        "                else:\n",
        "                    loss_repel = torch.zeros(1)\n",
        "\n",
        "            else:\n",
        "                loss_kl = torch.zeros(1)\n",
        "                loss_repel = torch.zeros(1)\n",
        "\n",
        "            if FLAGS.hmc:\n",
        "                v_flat = v.view(v.size(0), -1)\n",
        "                im_grad_flat = x_grad.view(x_grad.size(0), -1)\n",
        "                dot_product = F.normalize(v_flat, dim=1) * F.normalize(im_grad_flat, dim=1)\n",
        "                hmc_loss = torch.abs(dot_product.sum(dim=1)).mean()\n",
        "                loss = loss + 0.01 * hmc_loss\n",
        "            # else:\n",
        "            #     hmc_loss = torch.zeros(1)\n",
        "\n",
        "            if FLAGS.log_grad and len(replay_buffer) > 1000:\n",
        "                loss_kl = loss_kl - 0.1 * loss_repel\n",
        "                loss_kl = loss_kl.mean()\n",
        "                loss_ml = energy_pos.mean() - energy_neg.mean()\n",
        "\n",
        "                loss_ml.backward(retain_graph=True)\n",
        "                ele = []\n",
        "\n",
        "                for param in model.parameters():\n",
        "                    if param.grad is not None:\n",
        "                        ele.append(torch.norm(param.grad.data))\n",
        "\n",
        "                ele = torch.stack(ele, dim=0)\n",
        "                ml_grad = torch.mean(ele)\n",
        "                model.zero_grad()\n",
        "\n",
        "                loss_kl.backward(retain_graph=True) \n",
        "                ele = []\n",
        "\n",
        "                for param in model.parameters():\n",
        "                    if param.grad is not None:\n",
        "                        ele.append(torch.norm(param.grad.data))\n",
        "\n",
        "                ele = torch.stack(ele, dim=0)\n",
        "                kl_grad = torch.mean(ele)\n",
        "                model.zero_grad()\n",
        "\n",
        "            else:\n",
        "                ml_grad = None\n",
        "                kl_grad = None\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            clip_grad_norm_(model.parameters(), 0.5)\n",
        "\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # ema_model(models, models_ema)\n",
        "\n",
        "            if torch.isnan(energy_pos.mean()):\n",
        "                assert False\n",
        "\n",
        "            if torch.abs(energy_pos.mean()) > 10.0:\n",
        "                assert False\n",
        "\n",
        "            if itr % FLAGS.log_interval == 0:\n",
        "                tick = time.time()\n",
        "                kvs = {}\n",
        "                kvs['e_pos'] = energy_pos.mean().item()\n",
        "                kvs['e_pos_std'] = energy_pos.std().item()\n",
        "                kvs['e_neg'] = energy_neg.mean().item()\n",
        "                kvs['kl_mean'] = loss_kl.mean().item()\n",
        "                kvs['loss_repel'] = loss_repel.mean().item()\n",
        "                kvs['e_neg_std'] = energy_neg.std().item()\n",
        "                kvs['e_diff'] = kvs['e_pos'] - kvs['e_neg']\n",
        "                # kvs['x_grad'] = np.abs(x_grad.detach().cpu().numpy()).mean()\n",
        "                kvs['x_grad'] = x_grad\n",
        "                kvs['iter'] = itr\n",
        "                # kvs['hmc_loss'] = hmc_loss.item()\n",
        "                kvs['num_steps'] = num_steps\n",
        "                # kvs['t_diff'] = tick - tock\n",
        "                kvs['positive_samples'] = data.detach()\n",
        "                kvs['negative_samples'] = im_neg.detach()\n",
        "\n",
        "                if FLAGS.replay_batch:\n",
        "                    kvs['length'] = len(replay_buffer)\n",
        "\n",
        "                if (ml_grad is not None):\n",
        "                    kvs['kl_grad'] = kl_grad\n",
        "                    kvs['ml_grad'] = ml_grad\n",
        "\n",
        "                log_tensorboard(kvs)\n",
        "                tock = tick\n",
        "\n",
        "            if itr % FLAGS.save_interval == 0 and (FLAGS.save_interval != 0):\n",
        "                model_path = osp.join(logdir, \"model_{}.pth\".format(itr))\n",
        "                ckpt = {'optimizer_state_dict': optimizer.state_dict(),\n",
        "                            'FLAGS': FLAGS, 'best_inception': best_inception}\n",
        "\n",
        "                for i in range(FLAGS.ensembles):\n",
        "                    ckpt['model_state_dict_{}'.format(i)] = model.state_dict()\n",
        "                    # ckpt['ema_model_state_dict_{}'.format(i)] = model.state_dict()\n",
        "\n",
        "                torch.save(ckpt, model_path)\n",
        "\n",
        "            # if itr % FLAGS.save_interval == 0 and rank_idx == 0:\n",
        "            #     im_samples = im_samples[::10]\n",
        "            #     im_samples_total = torch.stack(im_samples, dim=1).detach().cpu().permute(0, 1, 3, 4, 2).numpy()\n",
        "            #     try_im = im_neg\n",
        "            #     orig_im = data_corrupt\n",
        "            #     actual_im = rescale_im(data.detach().permute(0, 2, 3, 1).cpu().numpy())\n",
        "\n",
        "            #     orig_im = rescale_im(orig_im.detach().permute(0, 2, 3, 1).cpu().numpy())\n",
        "            #     try_im = rescale_im(try_im.detach().permute(0, 2, 3, 1).cpu().numpy()).squeeze()\n",
        "            #     im_samples_total = rescale_im(im_samples_total)\n",
        "\n",
        "            #     if rank_idx == 0:\n",
        "            #         score, std = get_inception_score(list(try_im), splits=1)\n",
        "            #         print(\"Inception score of {} with std of {}\".format(\n",
        "            #                 score, std))\n",
        "            #         # kvs = {}\n",
        "            #         # kvs['inception_score'] = score\n",
        "            #         # kvs['inception_score_std'] = std\n",
        "            #         # logger.writekvs(kvs)\n",
        "            #         writer.add_scalar(\"inception score\", score, itr)\n",
        "            #         writer.add_scalar(\"inception score std\", std, itr)\n",
        "\n",
        "            #         if score > best_inception:\n",
        "            #             model_path = osp.join(logdir, \"model_best.pth\")\n",
        "            #             torch.save(ckpt, model_path)\n",
        "            #             best_inception = score\n",
        "\n",
        "            itr += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpUlfIX10JI7"
      },
      "source": [
        "def main_single(FLAGS):\n",
        "    print(\"Values of args: \", FLAGS)\n",
        "\n",
        "    if FLAGS.dataset == \"cifar10\":\n",
        "        train_dataset = Cifar10(FLAGS)\n",
        "        # valid_dataset = Cifar10(FLAGS, split='valid', augment=False)\n",
        "        # test_dataset = Cifar10(FLAGS, split='test', augment=False)\n",
        "    elif FLAGS.dataset == \"celeba\":\n",
        "        train_dataset = CelebADataset(FLAGS)\n",
        "        # valid_dataset = CelebADataset(FLAGS, train=False, augment=False)\n",
        "        # test_dataset = CelebADataset(FLAGS, train=False, augment=False)\n",
        "    elif FLAGS.dataset == \"cats\":\n",
        "        train_dataset = Cats()\n",
        "    elif FLAGS.dataset == \"stl\":\n",
        "        train_dataset = STLDataset(FLAGS)\n",
        "        # valid_dataset = STLDataset(FLAGS, train=False)\n",
        "        # test_dataset = STLDataset(FLAGS, train=False)\n",
        "    elif FLAGS.dataset == \"object\":\n",
        "        train_dataset = ObjectDataset(FLAGS.cond_idx)\n",
        "        # valid_dataset = ObjectDataset(FLAGS.cond_idx)\n",
        "        # test_dataset = ObjectDataset(FLAGS.cond_idx)\n",
        "    elif FLAGS.dataset == \"imagenet\":\n",
        "        train_dataset = ImageNet()\n",
        "        # valid_dataset = ImageNet()\n",
        "        # test_dataset = ImageNet()\n",
        "    elif FLAGS.dataset == \"mnist\":\n",
        "        train_dataset = Mnist(train=True)\n",
        "        # valid_dataset = Mnist(train=False)\n",
        "        # test_dataset = Mnist(train=False)\n",
        "    elif FLAGS.dataset == \"celebahq\":\n",
        "        train_dataset = CelebAHQ(cond_idx=FLAGS.cond_idx)\n",
        "        # valid_dataset = CelebAHQ(cond_idx=FLAGS.cond_idx)\n",
        "        # test_dataset = CelebAHQ(cond_idx=FLAGS.cond_idx)\n",
        "    elif FLAGS.dataset == \"lsun\":\n",
        "        train_dataset = LSUNBed(cond_idx=FLAGS.cond_idx)\n",
        "        # valid_dataset = LSUNBed(cond_idx=FLAGS.cond_idx)\n",
        "        # test_dataset = LSUNBed(cond_idx=FLAGS.cond_idx)\n",
        "    else:\n",
        "        assert False\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, num_workers=FLAGS.data_workers, batch_size=FLAGS.batch_size, shuffle=True, drop_last=True)\n",
        "    # valid_dataloader = DataLoader(valid_dataset, num_workers=FLAGS.data_workers, batch_size=FLAGS.batch_size, shuffle=True, drop_last=True)\n",
        "    # test_dataloader = DataLoader(test_dataset, num_workers=FLAGS.data_workers, batch_size=FLAGS.batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "    logdir = osp.join(sample_dir, FLAGS.exp, FLAGS.dataset)\n",
        "\n",
        "    best_inception = 0.0\n",
        "    \n",
        "    if FLAGS.resume_iter != 0:\n",
        "        FLAGS_OLD = FLAGS\n",
        "        model_path = osp.join(logdir, \"model_{}.pth\".format(FLAGS.resume_iter))\n",
        "        checkpoint = torch.load(model_path)\n",
        "        best_inception = checkpoint['best_inception']\n",
        "        FLAGS = checkpoint['FLAGS']\n",
        "\n",
        "        FLAGS.resume_iter = FLAGS_OLD.resume_iter\n",
        "        FLAGS_OLD = None\n",
        "\n",
        "    if FLAGS.dataset in (\"cifar10\", \"celeba\", \"cats\"):\n",
        "        model_fn = ResNetModel\n",
        "    elif FLAGS.dataset == \"stl\":\n",
        "        model_fn = ResNetModel\n",
        "    elif FLAGS.dataset == \"object\":\n",
        "        model_fn = CelebAModel\n",
        "    elif FLAGS.dataset == \"mnist\":\n",
        "        model_fn = MNISTModel\n",
        "    elif FLAGS.dataset == \"celebahq\":\n",
        "        model_fn = CelebAModel\n",
        "    elif FLAGS.dataset == \"lsun\":\n",
        "        model_fn = CelebAModel\n",
        "    elif FLAGS.dataset == \"imagenet\":\n",
        "        model_fn = ImagenetModel\n",
        "    else:\n",
        "        assert False\n",
        "\n",
        "    model = model_fn(FLAGS).train()\n",
        "    # models_ema = model_fn(FLAGS).train()\n",
        "\n",
        "    optimizer = Adam(model.parameters(), lr=FLAGS.lr, betas=(0.0, 0.9), eps=1e-8)\n",
        "\n",
        "    # ema_model(models, models_ema, mu=0.0)\n",
        "\n",
        "    it = FLAGS.resume_iter\n",
        "\n",
        "    if not osp.exists(logdir):\n",
        "        os.makedirs(logdir)\n",
        "\n",
        "    checkpoint = None\n",
        "    if FLAGS.resume_iter != 0:\n",
        "        print(\"FLAGS.resume_iter:\",FLAGS.resume_iter)\n",
        "        model_path = osp.join(logdir, \"model_{iter}.pth\".format(FLAGS.resume_iter))\n",
        "        checkpoint = torch.load(model_path)\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "        for i in range(FLAGS.ensembles):\n",
        "            model.load_state_dict(checkpoint['model_state_dict_{}'.format(i)])\n",
        "            # model_ema.load_state_dict(checkpoint['ema_model_state_dict_{}'.format(i)])\n",
        " \n",
        "\n",
        "    print(\"New Values of args: \", FLAGS)\n",
        "\n",
        "    pytorch_total_params = sum([p.numel() for p in model.parameters() if p.requires_grad])\n",
        "    print(\"Number of parameters for models\", pytorch_total_params)\n",
        "\n",
        "    train(model, optimizer, train_dataloader, logdir, FLAGS.resume_iter, FLAGS, best_inception)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-fgH7DbHm39"
      },
      "source": [
        "# Call this function with list of images. Each of elements should be a \n",
        "# numpy array with values ranging from 0 to 255.\n",
        "def get_inception_score(images, splits=10):\n",
        "  return 0.0, 0.0\n",
        "  # For convenience\n",
        "  if len(images[0].shape) != 3:\n",
        "    return 0, 0\n",
        "\n",
        "  # Bypassing all the assertions so that we don't end prematuraly'\n",
        "  # assert(type(images) == list)\n",
        "  # assert(type(images[0]) == np.ndarray)\n",
        "  # assert(len(images[0].shape) == 3)\n",
        "  # assert(np.max(images[0]) > 10)\n",
        "  # assert(np.min(images[0]) >= 0.0)\n",
        "  inps = []\n",
        "  for img in images:\n",
        "    img = img.astype(np.float32)\n",
        "    inps.append(np.expand_dims(img, 0))\n",
        "  bs = 1\n",
        "  preds = []\n",
        "  n_batches = int(math.ceil(float(len(inps)) / float(bs)))\n",
        "  for i in range(n_batches):\n",
        "      # sys.stdout.write(\".\")\n",
        "      # sys.stdout.flush()\n",
        "      inp = inps[(i * bs):min((i + 1) * bs, len(inps))]\n",
        "      inp = np.concatenate(inp, 0)\n",
        "      pred = sess.run(softmax, {'ExpandDims:0': inp})\n",
        "      preds.append(pred)\n",
        "  preds = np.concatenate(preds, 0)\n",
        "  scores = []\n",
        "  for i in range(splits):\n",
        "    part = preds[(i * preds.shape[0] // splits):((i + 1) * preds.shape[0] // splits), :]\n",
        "    kl = part * (np.log(part) - np.log(np.expand_dims(np.mean(part, 0), 0)))\n",
        "    kl = np.mean(np.sum(kl, 1))\n",
        "    scores.append(np.exp(kl))\n",
        "  return np.mean(scores), np.std(scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftzvugDbDs-T"
      },
      "source": [
        " tensorboard --logdir runs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGcd_r9F0MqF"
      },
      "source": [
        "main_single(flags)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}